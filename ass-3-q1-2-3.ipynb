{"cells":[{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T21:05:23.765575Z","iopub.status.busy":"2024-04-20T21:05:23.764852Z","iopub.status.idle":"2024-04-20T21:05:23.771299Z","shell.execute_reply":"2024-04-20T21:05:23.770194Z","shell.execute_reply.started":"2024-04-20T21:05:23.765545Z"},"trusted":true},"outputs":[],"source":["import csv\n","import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable \n","import copy\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader, TensorDataset\n","from torch.nn.utils import clip_grad_norm_\n","import random"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T21:05:23.990175Z","iopub.status.busy":"2024-04-20T21:05:23.989521Z","iopub.status.idle":"2024-04-20T21:05:23.994901Z","shell.execute_reply":"2024-04-20T21:05:23.993920Z","shell.execute_reply.started":"2024-04-20T21:05:23.990145Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T21:05:24.295018Z","iopub.status.busy":"2024-04-20T21:05:24.294079Z","iopub.status.idle":"2024-04-20T21:05:24.312204Z","shell.execute_reply":"2024-04-20T21:05:24.311303Z","shell.execute_reply.started":"2024-04-20T21:05:24.294983Z"},"trusted":true},"outputs":[],"source":["def loadData(train_path, val_path, test_path):\n","    train_data = csv.reader(open(train_path,encoding='utf8'))\n","    val_data = csv.reader(open(val_path,encoding='utf8'))\n","    test_data = csv.reader(open(test_path,encoding='utf8'))\n","    train_words , train_translations = [], []\n","    val_words , val_translations = [], []\n","    test_words , test_translations = [], []\n","    \n","    empty, start, end ='', '^', '$'\n","    for pair in train_data:\n","        train_words.append(pair[0] + end)\n","        train_translations.append(start + pair[1] + end)\n","    for pair in val_data:\n","        val_words.append(pair[0] + end)\n","        val_translations.append(start + pair[1] + end)\n","    for pair in test_data:\n","        test_words.append(pair[0] + end)\n","        test_translations.append(start + pair[1] + end)\n","    \n","    train_words , train_translations = np.array(train_words), np.array(train_translations)\n","    val_words , val_translations = np.array(val_words), np.array(val_translations)\n","    test_words , test_translations = np.array(test_words), np.array(test_translations)\n","\n","    inout_vocab = set()\n","    output_vocab = set()\n","    \n","    for w in train_words:\n","        for c in w:\n","            inout_vocab.add(c)\n","    for w in val_words:\n","        for c in w:\n","            inout_vocab.add(c)\n","    for w in test_words:\n","        for c in w:\n","            inout_vocab.add(c)\n","            \n","    for w in train_translations:\n","        for c in w:\n","            output_vocab.add(c)\n","    for w in val_translations:\n","        for c in w:\n","            output_vocab.add(c)\n","    for w in test_translations:\n","        for c in w:\n","            output_vocab.add(c)\n","    \n","    inout_vocab.remove(end)\n","    output_vocab.remove(start)\n","    output_vocab.remove(end)  \n","    inout_vocab, output_vocab = [empty, start, end] + list(sorted(inout_vocab)), [empty, start, end] + list(sorted(output_vocab))\n","            \n","    input_index = dict([(char, idx) for idx, char in enumerate(inout_vocab)])\n","    output_index =  dict([(char, idx) for idx, char in enumerate(output_vocab)])\n","    input_index_rev = dict([(idx, char) for char, idx in input_index.items()])\n","    output_index_rev = dict([(idx, char) for char, idx in output_index.items()])\n","    \n","    max_enc_len = max([len(word) for word in np.hstack((train_words, test_words, val_words))])\n","    max_dec_len = max([len(word) for word in np.hstack((train_translations, val_translations, test_translations))])\n","    max_len = max(max_enc_len, max_dec_len)\n","        \n","    result = {\n","        'train_words' : train_words,\n","        'train_translations' : train_translations,\n","        'val_words' : val_words,\n","        'val_translations' : val_translations,\n","        'test_words' : test_words,\n","        'test_translations' : test_translations,\n","        'max_enc_len' : max_enc_len,\n","        'max_dec_len' : max_dec_len,\n","        'max_len' : max_len,\n","        'input_index' : input_index,\n","        'output_index' : output_index,\n","        'input_index_rev' : input_index_rev,\n","        'output_index_rev' : output_index_rev\n","    }\n","    return result"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T21:05:24.647729Z","iopub.status.busy":"2024-04-20T21:05:24.647355Z","iopub.status.idle":"2024-04-20T21:05:24.661950Z","shell.execute_reply":"2024-04-20T21:05:24.661021Z","shell.execute_reply.started":"2024-04-20T21:05:24.647699Z"},"trusted":true},"outputs":[],"source":["def create_tensor(result):\n","    input_data = np.zeros((result['max_len'],len(result['train_words'])), dtype = 'int64')\n","    output_data = np.zeros((result['max_len'],len(result['train_words'])), dtype = 'int64')\n","    \n","    val_input_data = np.zeros((result['max_len'],len(result['val_words'])), dtype = 'int64')\n","    val_output_data = np.zeros((result['max_len'],len(result['val_words'])), dtype = 'int64')\n","    \n","    test_input_data = np.zeros((result['max_len'],len(result['test_words'])), dtype = 'int64')\n","    test_output_data = np.zeros((result['max_len'],len(result['test_words'])), dtype = 'int64')\n","    \n","    for idx, (w, t) in enumerate(zip(result['train_words'], result['train_translations'])):\n","        for i, char in enumerate(w):\n","            input_data[i, idx] = result['input_index'][char]\n","        for i, char in enumerate(t):\n","            output_data[i, idx] = result['output_index'][char]\n","        \n","    for idx, (w, t) in enumerate(zip(result['val_words'], result['val_translations'])):\n","        for i, char in enumerate(w):\n","            val_input_data[i, idx] = result['input_index'][char]\n","        for i, char in enumerate(t):\n","            val_output_data[i, idx] = result['output_index'][char]\n","    \n","    for idx, (w, t) in enumerate(zip(result['test_words'], result['test_translations'])):\n","        for i, char in enumerate(w):\n","            test_input_data[i, idx] = result['input_index'][char]\n","        for i, char in enumerate(t):\n","            test_output_data[i, idx] = result['output_index'][char]\n","    \n","    input_data, output_data = torch.tensor(input_data,dtype = torch.int64), torch.tensor(output_data, dtype = torch.int64)\n","    val_input_data, val_output_data = torch.tensor(val_input_data,dtype = torch.int64), torch.tensor(val_output_data, dtype = torch.int64)\n","    test_input_data, test_output_data = torch.tensor(test_input_data,dtype = torch.int64), torch.tensor(test_output_data, dtype = torch.int64)\n","    \n","    tensors = {\n","        'input_data' : input_data,\n","        'output_data' : output_data,\n","        'val_input_data' : val_input_data,\n","        'val_output_data' : val_output_data, \n","        'test_input_data' : test_input_data,\n","        'test_output_data' : test_output_data\n","    }\n","    return tensors"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T21:05:24.989973Z","iopub.status.busy":"2024-04-20T21:05:24.989115Z","iopub.status.idle":"2024-04-20T21:05:26.045032Z","shell.execute_reply":"2024-04-20T21:05:26.043138Z","shell.execute_reply.started":"2024-04-20T21:05:24.989941Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Input data :  ['shastragaar$' 'bindhya$' 'kirankant$' ... 'asahmaton$' 'sulgaayin$'\n"," 'anchuthengu$']\n","Output data :  ['^शस्त्रागार$' '^बिन्द्या$' '^किरणकांत$' ... '^असहमतों$' '^सुलगायीं$'\n"," '^अंचुतेंगु$']\n","Number of samples :  51200\n","Input data :  ['jaisawal$' 'bajai$' 'sanghthan$' ... 'ekamreshwar$' 'bluetooth$'\n"," 'govindram$']\n","Output data :  ['^जयसवाल$' '^बजाई$' '^संघठन$' ... '^एकाम्रेश्वर$' '^ब्ल्यूटूथ$'\n"," '^गोविंद्राम$']\n","Number of val samples :  4096\n","Input data :  ['thermax$' 'sikhaaega$' 'learn$' ... 'khaatootolaa$' 'shivastava$'\n"," 'preranapuree$']\n","Output data :  ['^थरमैक्स$' '^सिखाएगा$' '^लर्न$' ... '^खातूटोला$' '^शिवास्तव$'\n"," '^प्रेरणापुरी$']\n","Number of test samples :  4096\n","Max incoder length :  27\n","Max decoder length :  22\n","Input index length 29\n","Output index length 68\n","Input index {'': 0, '^': 1, '$': 2, 'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'j': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'p': 18, 'q': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28}\n","Output index {'': 0, '^': 1, '$': 2, 'ँ': 3, 'ं': 4, 'ः': 5, 'अ': 6, 'आ': 7, 'इ': 8, 'ई': 9, 'उ': 10, 'ऊ': 11, 'ऋ': 12, 'ए': 13, 'ऐ': 14, 'ऑ': 15, 'ओ': 16, 'औ': 17, 'क': 18, 'ख': 19, 'ग': 20, 'घ': 21, 'ङ': 22, 'च': 23, 'छ': 24, 'ज': 25, 'झ': 26, 'ञ': 27, 'ट': 28, 'ठ': 29, 'ड': 30, 'ढ': 31, 'ण': 32, 'त': 33, 'थ': 34, 'द': 35, 'ध': 36, 'न': 37, 'प': 38, 'फ': 39, 'ब': 40, 'भ': 41, 'म': 42, 'य': 43, 'र': 44, 'ल': 45, 'ळ': 46, 'व': 47, 'श': 48, 'ष': 49, 'स': 50, 'ह': 51, '़': 52, 'ऽ': 53, 'ा': 54, 'ि': 55, 'ी': 56, 'ु': 57, 'ू': 58, 'ृ': 59, 'ॅ': 60, 'े': 61, 'ै': 62, 'ॉ': 63, 'ॊ': 64, 'ो': 65, 'ौ': 66, '्': 67}\n","Input index Rev {0: '', 1: '^', 2: '$', 3: 'a', 4: 'b', 5: 'c', 6: 'd', 7: 'e', 8: 'f', 9: 'g', 10: 'h', 11: 'i', 12: 'j', 13: 'k', 14: 'l', 15: 'm', 16: 'n', 17: 'o', 18: 'p', 19: 'q', 20: 'r', 21: 's', 22: 't', 23: 'u', 24: 'v', 25: 'w', 26: 'x', 27: 'y', 28: 'z'}\n","Output index Rev {0: '', 1: '^', 2: '$', 3: 'ँ', 4: 'ं', 5: 'ः', 6: 'अ', 7: 'आ', 8: 'इ', 9: 'ई', 10: 'उ', 11: 'ऊ', 12: 'ऋ', 13: 'ए', 14: 'ऐ', 15: 'ऑ', 16: 'ओ', 17: 'औ', 18: 'क', 19: 'ख', 20: 'ग', 21: 'घ', 22: 'ङ', 23: 'च', 24: 'छ', 25: 'ज', 26: 'झ', 27: 'ञ', 28: 'ट', 29: 'ठ', 30: 'ड', 31: 'ढ', 32: 'ण', 33: 'त', 34: 'थ', 35: 'द', 36: 'ध', 37: 'न', 38: 'प', 39: 'फ', 40: 'ब', 41: 'भ', 42: 'म', 43: 'य', 44: 'र', 45: 'ल', 46: 'ळ', 47: 'व', 48: 'श', 49: 'ष', 50: 'स', 51: 'ह', 52: '़', 53: 'ऽ', 54: 'ा', 55: 'ि', 56: 'ी', 57: 'ु', 58: 'ू', 59: 'ृ', 60: 'ॅ', 61: 'े', 62: 'ै', 63: 'ॉ', 64: 'ॊ', 65: 'ो', 66: 'ौ', 67: '्'}\n","Input Data torch.Size([27, 51200])\n","Output Data torch.Size([27, 51200])\n","Input Data Val torch.Size([27, 4096])\n","Output Data Val torch.Size([27, 4096])\n","Input Data Test torch.Size([27, 4096])\n","Output Data Test torch.Size([27, 4096])\n"]}],"source":["language = 'hin'\n","# dataset_path = r'C:\\Users\\gragh\\OneDrive\\Desktop\\Codes\\CS6910 DL\\Assignment 3\\DataSet\\aksharantar_sampled'\n","dataset_path = '/kaggle/input/dl-ass3/aksharantar_sampled'\n","\n","train_path = os.path.join(dataset_path, language, language + '_train.csv')\n","val_path = os.path.join(dataset_path, language, language + '_valid.csv')\n","test_path = os.path.join(dataset_path, language, language + '_test.csv')\n","result = loadData(train_path, val_path, test_path)\n","tensors = create_tensor(result)\n","\n","print('Input data : ', result['train_words'])\n","print('Output data : ', result['train_translations'])\n","print('Number of samples : ', len(result['train_words']))\n","\n","print('Input data : ', result['val_words'])\n","print('Output data : ', result['val_translations'])\n","print('Number of val samples : ', len(result['val_words']))\n","\n","print('Input data : ', result['test_words'])\n","print('Output data : ', result['test_translations'])\n","print('Number of test samples : ', len(result['test_words']))\n","\n","print('Max incoder length : ', result['max_enc_len'])\n","print('Max decoder length : ', result['max_dec_len'])\n","\n","print('Input index length', len(result['input_index']))\n","print('Output index length', len(result['output_index']))\n","print('Input index', result['input_index'])\n","print('Output index', result['output_index'])\n","print('Input index Rev', result['input_index_rev'])\n","print('Output index Rev', result['output_index_rev'])\n","\n","print('Input Data', tensors['input_data'].shape)\n","print('Output Data', tensors['output_data'].shape)\n","print('Input Data Val', tensors['val_input_data'].shape)\n","print('Output Data Val', tensors['val_output_data'].shape)\n","print('Input Data Test', tensors['test_input_data'].shape)\n","print('Output Data Test', tensors['test_output_data'].shape)\n","\n","# print(tensors['input_data'][:,0])\n","# print(tensors['output_data'][:,0])"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T21:46:48.065093Z","iopub.status.busy":"2024-04-20T21:46:48.064759Z","iopub.status.idle":"2024-04-20T21:46:48.082709Z","shell.execute_reply":"2024-04-20T21:46:48.081652Z","shell.execute_reply.started":"2024-04-20T21:46:48.065068Z"},"trusted":true},"outputs":[],"source":["class Encoder(nn.Module): \n","    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n","        super(Encoder, self).__init__()\n","        drop_par = dropout\n","        self.dropout = nn.Dropout(drop_par)\n","        self.num_layers, self.hidden_size, = num_layers, hidden_size\n","        self.embedding = nn.Embedding(input_size, embedding_size)\n","        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout)\n","\n","    def forward(self, x):\n","        drop_par = self.embedding(x)\n","        outputs, (hidden, cell) = self.rnn(self.dropout(drop_par))\n","        return hidden, cell\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout):\n","        super(Decoder, self).__init__()\n","        drop_par, hidden_layer_size = dropout, hidden_size\n","        self.dropout,self.num_layers, self.hidden_size = nn.Dropout(drop_par),  num_layers, hidden_layer_size\n","        self.embedding = nn.Embedding(input_size, embedding_size)\n","        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout= drop_par)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x, hidden, cell):\n","        x  = x.unsqueeze(0) \n","        embedding = self.embedding(x)\n","        outputs, (hidden, cell) = self.rnn(self.dropout(embedding), (hidden, cell))\n","        predictions = self.fc(outputs).squeeze(0)\n","        predictions = F.log_softmax(predictions, dim = 1)\n","        return predictions, hidden, cell\n","    \n","class Seq2Seq(nn.Module):\n","\n","    def __init__(self, encoder, decoder, output_index_len):\n","        super(Seq2Seq, self).__init__()  \n","        self.decoder, self.encoder = decoder, encoder\n","        self.output_index_len = output_index_len\n","\n","    def forward(self, source, target, teacher_force_ratio=0.5):\n","        batch_size, target_len, target_vocab_size = source.shape[1], target.shape[0], self.output_index_len\n","        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n","        x = target[0]\n","        hidden, cell = self.encoder(source)\n","\n","        for t in range(1, target_len):\n","            output, hidden, cell = self.decoder(x, hidden, cell)\n","            outputs[t], best_guess = output, output.argmax(1)\n","            x = best_guess if random.random() >= teacher_force_ratio else target[t]\n","        return outputs"]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T21:46:48.556052Z","iopub.status.busy":"2024-04-20T21:46:48.555722Z","iopub.status.idle":"2024-04-20T21:46:48.564317Z","shell.execute_reply":"2024-04-20T21:46:48.563480Z","shell.execute_reply.started":"2024-04-20T21:46:48.556028Z"},"trusted":true},"outputs":[],"source":["# def evaluate(model, val_loader, criterion):\n","#     model.eval()\n","#     total_loss = 0\n","#     correct_predictions = 0\n","#     total_predictions = 0\n","    \n","#     with torch.no_grad():\n","#         for source, target in val_loader:\n","#             source = source.to(device)\n","#             target = target.to(device)\n","#             output = model(source, target, 0)\n","#             output_dim = output.shape[-1]\n","#             output = output[1:].view(-1, output_dim)\n","#             target = target[1:].view(-1)\n","#             loss = criterion(output, target)\n","#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1)\n","#             total_loss += loss.item()\n","#             predicted = output.argmax(dim=1)\n","#             correct_predictions += (predicted == target).sum().item()\n","#             total_predictions += target.numel()\n","\n","#     val_loss = total_loss / len(val_loader)\n","#     val_accuracy = correct_predictions / total_predictions\n","\n","#     return val_loss, val_accuracy\n","\n","# def Train(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n","#     for epoch in range(num_epochs):\n","#         model.train()\n","#         total_loss = 0\n","#         correct_predictions = 0\n","#         total_predictions = 0\n","\n","#         for source, target in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n","#             source = source.to(device)\n","#             target = target.to(device)\n","\n","#             optimizer.zero_grad()\n","#             output = model(source, target)\n","#             output_dim = output.shape[-1]\n","\n","#             output = output[1:].view(-1, output_dim)\n","#             target = target[1:].view(-1)\n","\n","#             loss = criterion(output, target)\n","\n","#             loss.backward()\n","#             optimizer.step()\n","\n","#             total_loss += loss.item()\n","\n","#             # Calculate accuracy\n","#             predicted = output.argmax(dim=1)\n","#             correct_predictions += (predicted == target).sum().item()\n","#             total_predictions += target.numel()\n","\n","#         train_loss = total_loss / len(train_loader)\n","#         train_accuracy = correct_predictions / total_predictions\n","#         val_loss, val_accuracy = evaluate(model, val_loader, criterion)\n","#         print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f} Val Accuracy: {val_accuracy:.4f}\")    \n","\n","#     return model"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T21:52:28.501243Z","iopub.status.busy":"2024-04-20T21:52:28.500234Z","iopub.status.idle":"2024-04-20T22:02:28.348113Z","shell.execute_reply":"2024-04-20T22:02:28.347199Z","shell.execute_reply.started":"2024-04-20T21:52:28.501207Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Trainable Parameters: 7416132\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1600/1600 [00:58<00:00, 27.54it/s]\n","Validation: 100%|██████████| 128/128 [00:01<00:00, 78.38it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1 Train Accuracy: 34.3797, Train Loss: 2.4545 Validation Accuracy: 57.7149, Validation  Loss: 1.5050\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1600/1600 [00:58<00:00, 27.47it/s]\n","Validation: 100%|██████████| 128/128 [00:01<00:00, 78.60it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2 Train Accuracy: 58.4598, Train Loss: 1.4433 Validation Accuracy: 65.6898, Validation  Loss: 1.2402\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1600/1600 [00:58<00:00, 27.37it/s]\n","Validation: 100%|██████████| 128/128 [00:01<00:00, 79.26it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3 Train Accuracy: 64.5703, Train Loss: 1.2346 Validation Accuracy: 68.0348, Validation  Loss: 1.1656\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1600/1600 [00:58<00:00, 27.52it/s]\n","Validation: 100%|██████████| 128/128 [00:01<00:00, 68.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4 Train Accuracy: 67.4981, Train Loss: 1.1421 Validation Accuracy: 69.2716, Validation  Loss: 1.1195\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1600/1600 [00:57<00:00, 27.63it/s]\n","Validation: 100%|██████████| 128/128 [00:01<00:00, 77.32it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5 Train Accuracy: 69.3933, Train Loss: 1.0812 Validation Accuracy: 70.3799, Validation  Loss: 1.0945\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1600/1600 [00:58<00:00, 27.49it/s]\n","Validation: 100%|██████████| 128/128 [00:01<00:00, 78.98it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6 Train Accuracy: 70.7724, Train Loss: 1.0375 Validation Accuracy: 71.0626, Validation  Loss: 1.0788\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1600/1600 [00:58<00:00, 27.55it/s]\n","Validation: 100%|██████████| 128/128 [00:01<00:00, 77.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7 Train Accuracy: 72.0694, Train Loss: 0.9984 Validation Accuracy: 71.2911, Validation  Loss: 1.0781\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1600/1600 [00:58<00:00, 27.44it/s]\n","Validation: 100%|██████████| 128/128 [00:01<00:00, 75.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8 Train Accuracy: 72.9594, Train Loss: 0.9691 Validation Accuracy: 71.6795, Validation  Loss: 1.0694\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1600/1600 [00:58<00:00, 27.36it/s]\n","Validation: 100%|██████████| 128/128 [00:01<00:00, 76.75it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9 Train Accuracy: 73.9322, Train Loss: 0.9392 Validation Accuracy: 72.2622, Validation  Loss: 1.0471\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1600/1600 [00:58<00:00, 27.55it/s]\n","Validation: 100%|██████████| 128/128 [00:01<00:00, 76.73it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 10 Train Accuracy: 74.3781, Train Loss: 0.9246 Validation Accuracy: 72.1480, Validation  Loss: 1.0601\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["language = 'hin'\n","# dataset_path = r'C:\\Users\\gragh\\OneDrive\\Desktop\\Codes\\CS6910 DL\\Assignment 3\\DataSet\\aksharantar_sampled'\n","dataset_path = '/kaggle/input/dl-ass3/aksharantar_sampled'\n","\n","train_path = os.path.join(dataset_path, language, language + '_train.csv')\n","val_path = os.path.join(dataset_path, language, language + '_valid.csv')\n","test_path = os.path.join(dataset_path, language, language + '_test.csv')\n","result = loadData(train_path, val_path, test_path)\n","tensors = create_tensor(result)\n","\n","params = {\n","    \"input_size\": len(result['input_index']),\n","    \"output_size\": len(result['output_index']),\n","    \"embedding_size\": 256,\n","    \"hidden_size\": 512,\n","    \"num_layers\": 2,\n","    \"cell_type\": \"LSTM\",\n","    \"dropout\": 0.5,\n","    \"learning_rate\": 0.001,\n","    \"batch_size\": 64,\n","    \"num_epochs\": 10\n","}\n","\n","encoder = Encoder(params['input_size'], params['embedding_size'], params['hidden_size'], params['num_layers'], params['dropout']).to(device)\n","decoder = Decoder(params['output_size'], params['embedding_size'], params['hidden_size'], params['output_size'], params['num_layers'], params['dropout']).to(device)\n","\n","model = Seq2Seq(encoder, decoder, len(result['output_index'])).to(device)\n","# print(model)\n","\n","# Print total number of parameters in the model\n","total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(f'Total Trainable Parameters: {total_params}')\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n","\n","train_ds_x, train_ds_y = torch.split(tensors['input_data'], 32, dim = 1), torch.split(tensors['output_data'], 32, dim = 1)\n","val_ds_x, val_ds_y = torch.split(tensors['val_input_data'], 32, dim=1), torch.split(tensors['val_output_data'], 32, dim=1)\n","\n","# correct_prediction = 0\n","\n","for epoch in range(params['num_epochs']):\n","  total_words = 0\n","  correct_pred = 0\n","  total_loss = 0\n","  model.train()\n","  with tqdm(total=len(train_ds_x), desc='Training') as pbar:\n","    for i, (x, y) in enumerate(zip(train_ds_x, train_ds_y)):\n","      target, inp_data = y.to(device), x.to(device)\n","      output = model(inp_data, target)\n","      pad_mask = (target != result['output_index'][''])  # Replace PAD_INDEX with your actual padding character index\n","      non_pad_targets = target[pad_mask]  # Select non-padding elements\n","      non_pad_outputs = output[pad_mask].reshape(-1, output.shape[2])  # Select corresponding outputs\n","      loss = criterion(non_pad_outputs, non_pad_targets)\n","      optimizer.zero_grad()\n","      loss.backward()\n","      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","      optimizer.step()\n","      total_words += non_pad_targets.size(0)\n","      correct_pred += torch.sum(torch.argmax(non_pad_outputs, dim=1) == non_pad_targets).item()\n","      total_loss += loss.item()\n","      pbar.update(1)\n","\n","  accuracy = correct_pred / total_words\n","  avg_loss = total_loss / len(train_ds_x)  # Average loss per batch\n","\n","  model.eval()\n","  with torch.no_grad():\n","    val_total_loss = 0\n","    val_total_words = 0\n","    val_correct_pred = 0\n","    with tqdm(total=len(val_ds_x), desc = 'Validation') as pbar:\n","      for x_val, y_val in zip(val_ds_x, val_ds_y):\n","        target_val, inp_data_val = y_val.to(device), x_val.to(device)\n","        output_val = model(inp_data_val, target_val)\n","        pad_mask = (target_val != result['output_index'][''])\n","        non_pad_targets = target_val[pad_mask]\n","        non_pad_outputs = output_val[pad_mask].reshape(-1, output_val.shape[2])\n","        val_loss = criterion(non_pad_outputs, non_pad_targets)\n","        val_total_loss += val_loss.item()\n","        val_total_words += non_pad_targets.size(0)\n","        val_correct_pred += torch.sum(torch.argmax(non_pad_outputs, dim=1) == non_pad_targets).item()\n","        pbar.update(1)\n","\n","    val_accuracy = val_correct_pred / val_total_words\n","    val_avg_loss = val_total_loss / len(val_ds_x)\n","    print(f\"Epoch {epoch+1} Train Accuracy: {accuracy*100:.4f}, Train Loss: {avg_loss:.4f} Validation Accuracy: {val_accuracy*100:.4f}, Validation Loss: {val_avg_loss:.4f}\")\n"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T22:02:28.350879Z","iopub.status.busy":"2024-04-20T22:02:28.350088Z","iopub.status.idle":"2024-04-20T22:02:28.359224Z","shell.execute_reply":"2024-04-20T22:02:28.358491Z","shell.execute_reply.started":"2024-04-20T22:02:28.350841Z"},"trusted":true},"outputs":[],"source":["def predict(model, word, input_char_index, output_char_index, reverse_target_char_index):\n","    data, word_t = np.zeros((len(input_char_index),1), dtype= int), ''\n","    t_z = 0\n","    for t, char in enumerate(word):\n","        data[t, 0] = input_char_index[char]\n","    t_z = t+1   \n","    data[t_z :,0] = input_char_index[\"$\"]\n","    data = torch.tensor(data,dtype = torch.int64).to(device)\n","    with torch.no_grad():\n","        hidden, cell = model.encoder(data)\n","    out_t = output_char_index['^']    \n","    out_chr_reshape = np.array(out_t).reshape(1,)    \n","    x = torch.tensor(out_chr_reshape).to(device)\n","\n","    for t in range(1, len(output_char_index)):\n","        output, hidden, cell = model.decoder(x, hidden, cell)\n","        ch = reverse_target_char_index[output.argmax(1).item()]\n","        if ch != '$':\n","            word_t = word_t+ch\n","        else:\n","            break\n","    return word_t"]},{"cell_type":"code","execution_count":87,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T22:10:34.684728Z","iopub.status.busy":"2024-04-20T22:10:34.684346Z","iopub.status.idle":"2024-04-20T22:10:34.820561Z","shell.execute_reply":"2024-04-20T22:10:34.819639Z","shell.execute_reply.started":"2024-04-20T22:10:34.684697Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["shastragaar -> शसस्््ररगग\n","bindhya -> बिंधध््\n","kirankant -> किरनकककं्\n","yagyopaveet -> यजगञयोपपिितत\n","ratania -> रताानिि\n","vaganyache -> वागण््येेे\n","deshbharamadhye -> देशभभररमम्््े\n","sughadpan -> सुघाडपप\n","mohiwal -> मोहिववल\n","sarvasangrah -> सररवससंगग््््\n","jaisawal -> जैसवालल\n","bajai -> बजाज\n","sanghthan -> संंघठन\n","haiwaan -> हैववव\n","nilgiri -> निललििररररर\n","drutgrami -> द्रटटररररररररर\n","jhadapon -> झडडपोंंं\n","nakronda -> नककरररंडड\n","eesl -> ईससएल\n","bachta -> बचचतत\n"]}],"source":["words = ['harsh', 'iit', 'madras', 'nirav', 'nidhi', 'nishchal', 'nishant', 'neymar', 'neha', 'raghav', 'rahul', 'rohit', 'hahahahaha', 'ohohohoh']\n","for w in result['train_words'][:10]:\n","    output_sequence = predict(model, w[:-1], result['input_index'], result['output_index'], result['output_index_rev'])\n","    print(w[:-1],\"->\",output_sequence)\n","for w in result['val_words'][:10]:\n","    output_sequence = predict(model, w[:-1], result['input_index'], result['output_index'], result['output_index_rev'])\n","    print(w[:-1],\"->\",output_sequence)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4831861,"sourceId":8165698,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
