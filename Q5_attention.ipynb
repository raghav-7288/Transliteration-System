{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# running py using bash\n","# !python /kaggle/input/train-py/train_attention.py -dp /kaggle/input/dl-ass3/aksharantar_sampled -lg hin"]},{"cell_type":"markdown","metadata":{},"source":["# IMPORTS"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:44:58.400880Z","iopub.status.busy":"2024-04-30T04:44:58.399926Z","iopub.status.idle":"2024-04-30T04:45:02.017421Z","shell.execute_reply":"2024-04-30T04:45:02.016508Z","shell.execute_reply.started":"2024-04-30T04:44:58.400833Z"},"trusted":true},"outputs":[],"source":["# adding necessary imports\n","import csv\n","import numpy as np\n","import pandas as pd\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import heapq\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","from torch.nn.utils import clip_grad_norm_\n","import matplotlib.pyplot as plt\n","from matplotlib.font_manager import FontProperties\n","import seaborn as sns\n","import random\n","import wandb\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:45:02.019631Z","iopub.status.busy":"2024-04-30T04:45:02.019299Z","iopub.status.idle":"2024-04-30T04:45:02.051240Z","shell.execute_reply":"2024-04-30T04:45:02.050209Z","shell.execute_reply.started":"2024-04-30T04:45:02.019599Z"},"trusted":true},"outputs":[],"source":["# Setting devide to gpu if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:45:02.053249Z","iopub.status.busy":"2024-04-30T04:45:02.052868Z","iopub.status.idle":"2024-04-30T04:45:04.936110Z","shell.execute_reply":"2024-04-30T04:45:04.935077Z","shell.execute_reply.started":"2024-04-30T04:45:02.053216Z"},"trusted":true},"outputs":[],"source":["# login to wandb\n","!wandb login 3c81526a5ec348850a4c9d0f852f6631959307ed"]},{"cell_type":"markdown","metadata":{},"source":["# PREPROCESSING"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:45:06.138254Z","iopub.status.busy":"2024-04-30T04:45:06.137488Z","iopub.status.idle":"2024-04-30T04:45:06.160198Z","shell.execute_reply":"2024-04-30T04:45:06.159067Z","shell.execute_reply.started":"2024-04-30T04:45:06.138211Z"},"trusted":true},"outputs":[],"source":["def loadData(params):\n","    \"\"\"\n","    This function loads and preprocesses the data for machine translation.\n","\n","    Args:\n","        params (dict): A dictionary containing parameters for data loading.\n","            - 'language': The language of the dataset (e.g., 'en', 'fr').\n","            - 'dataset_path': The path to the directory containing the dataset.\n","\n","    Returns:\n","        dict: A dictionary containing the preprocessed data.\n","    \"\"\"\n","    language = params['language']\n","    dataset_path = params['dataset_path']\n","    # Construct file paths for training, validation, and testing data\n","    train_path = os.path.join(dataset_path, language, language + '_train.csv')\n","    val_path = os.path.join(dataset_path, language, language + '_valid.csv')\n","    test_path = os.path.join(dataset_path, language, language + '_test.csv')\n","\n","    # Open and read data from CSV files using UTF-8 encoding for proper character handling\n","    train_data = csv.reader(open(train_path, encoding='utf8'))\n","    val_data = csv.reader(open(val_path, encoding='utf8'))\n","    test_data = csv.reader(open(test_path, encoding='utf8'))\n","\n","    # Initialize empty lists to store source and target language sentences\n","    train_words, train_translations = [], []\n","    val_words, val_translations = [], []\n","    test_words, test_translations = [], []\n","\n","    # Define special symbols for padding, sentence start, and sentence end\n","    pad, start, end = '', '^', '$'\n","\n","    # Preprocess data by adding special symbols to sentence ends\n","    for pair in train_data:\n","        train_words.append(pair[0] + end)\n","        train_translations.append(start + pair[1] + end)\n","    for pair in val_data:\n","        val_words.append(pair[0] + end)\n","        val_translations.append(start + pair[1] + end)\n","    for pair in test_data:\n","        test_words.append(pair[0] + end)\n","        test_translations.append(start + pair[1] + end)\n","    \n","    # Convert lists to NumPy arrays for efficient processing\n","    train_words , train_translations = np.array(train_words), np.array(train_translations)\n","    val_words , val_translations = np.array(val_words), np.array(val_translations)\n","    test_words , test_translations = np.array(test_words), np.array(test_translations)\n","    \n","    # Create sets to store unique characters in source and target vocabulary\n","    input_vocab = set()\n","    output_vocab = set()\n","    \n","    # Iterate through words to collect all unique characters\n","    for w in train_words:\n","        for c in w:\n","            input_vocab.add(c)\n","    for w in val_words:\n","        for c in w:\n","            input_vocab.add(c)\n","    for w in test_words:\n","        for c in w:\n","            input_vocab.add(c)\n","            \n","    for w in train_translations:\n","        for c in w:\n","            output_vocab.add(c)\n","    for w in val_translations:\n","        for c in w:\n","            output_vocab.add(c)\n","    for w in test_translations:\n","        for c in w:\n","            output_vocab.add(c)\n","    \n","    # Remove special symbols from vocabulary sets\n","    input_vocab.remove(end)\n","    output_vocab.remove(start)\n","    output_vocab.remove(end)\n","\n","    # Sort vocabulary sets and add special symbols as prefixes\n","    input_vocab, output_vocab = [pad, start, end] + list(sorted(input_vocab)), [pad, start, end] + list(sorted(output_vocab))\n","\n","    # Create dictionaries to map characters to their indices and vice versa\n","    input_index = {char: idx for idx, char in enumerate(input_vocab)}\n","    output_index = {char: idx for idx, char in enumerate(output_vocab)}\n","    input_index_rev = {idx: char for char, idx in input_index.items()}\n","    output_index_rev = {idx: char for char, idx in output_index.items()}\n","\n","    # Find the maximum length of sentences in source and target data\n","    max_enc_len = max([len(word) for word in np.hstack((train_words, test_words, val_words))])\n","    max_dec_len = max([len(word) for word in np.hstack((train_translations, val_translations, test_translations))])\n","    max_len = max(max_enc_len, max_dec_len)\n","      \n","    # returning data\n","    preprocessed_data = {\n","        'SOS' : start,\n","        'EOS' : end,\n","        'PAD' : pad,\n","        'train_words' : train_words,\n","        'train_translations' : train_translations,\n","        'val_words' : val_words,\n","        'val_translations' : val_translations,\n","        'test_words' : test_words,\n","        'test_translations' : test_translations,\n","        'max_enc_len' : max_enc_len,\n","        'max_dec_len' : max_dec_len,\n","        'max_len' : max_len,\n","        'input_index' : input_index,\n","        'output_index' : output_index,\n","        'input_index_rev' : input_index_rev,\n","        'output_index_rev' : output_index_rev\n","    }\n","    return preprocessed_data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:45:06.185079Z","iopub.status.busy":"2024-04-30T04:45:06.184762Z","iopub.status.idle":"2024-04-30T04:45:06.200898Z","shell.execute_reply":"2024-04-30T04:45:06.199827Z","shell.execute_reply.started":"2024-04-30T04:45:06.185037Z"},"trusted":true},"outputs":[],"source":["def create_tensor(preprocessed_data):\n","    \"\"\"\n","    This function creates PyTorch tensors from the preprocessed data.\n","\n","    Args:\n","        preprocessed_data (dict): A dictionary containing the preprocessed data.\n","\n","    Returns:\n","        dict: A dictionary containing PyTorch tensors for training, validation, and testing data.\n","    \"\"\"\n","\n","    # Define the maximum sequence length based on preprocessed data\n","    max_len = preprocessed_data['max_len']\n","\n","    # Create empty NumPy arrays to store padded sequences for training, validation, and testing\n","    input_data = np.zeros((max_len, len(preprocessed_data['train_words'])), dtype='int64')\n","    output_data = np.zeros((max_len, len(preprocessed_data['train_words'])), dtype='int64')\n","    \n","    val_input_data = np.zeros((max_len, len(preprocessed_data['val_words'])), dtype='int64')\n","    val_output_data = np.zeros((max_len, len(preprocessed_data['val_words'])), dtype='int64')\n","    \n","    test_input_data = np.zeros((max_len, len(preprocessed_data['test_words'])), dtype='int64')\n","    test_output_data = np.zeros((max_len, len(preprocessed_data['test_words'])), dtype='int64')\n","\n","    # Iterate through training data and populate tensors with character indices\n","    for idx, (w, t) in enumerate(zip(preprocessed_data['train_words'], preprocessed_data['train_translations'])):\n","        for i, char in enumerate(w):\n","            input_data[i, idx] = preprocessed_data['input_index'][char]\n","        for i, char in enumerate(t):\n","            output_data[i, idx] = preprocessed_data['output_index'][char]\n","\n","    # Repeat the process for validation and testing data\n","    for idx, (w, t) in enumerate(zip(preprocessed_data['val_words'], preprocessed_data['val_translations'])):\n","        for i, char in enumerate(w):\n","            val_input_data[i, idx] = preprocessed_data['input_index'][char]\n","        for i, char in enumerate(t):\n","            val_output_data[i, idx] = preprocessed_data['output_index'][char]\n","\n","    for idx, (w, t) in enumerate(zip(preprocessed_data['test_words'], preprocessed_data['test_translations'])):\n","        for i, char in enumerate(w):\n","            test_input_data[i, idx] = preprocessed_data['input_index'][char]\n","        for i, char in enumerate(t):\n","            test_output_data[i, idx] = preprocessed_data['output_index'][char]\n","\n","    # Convert NumPy arrays to PyTorch tensors for efficient GPU processing (if available)\n","    input_data, output_data = torch.tensor(input_data, dtype=torch.int64), torch.tensor(output_data, dtype=torch.int64)\n","    val_input_data, val_output_data = torch.tensor(val_input_data, dtype=torch.int64), torch.tensor(val_output_data, dtype=torch.int64)\n","    test_input_data, test_output_data = torch.tensor(test_input_data, dtype=torch.int64), torch.tensor(test_output_data, dtype=torch.int64)\n","\n","    # Create a dictionary to store all the tensors\n","    tensors = {\n","        'input_data': input_data,\n","        'output_data': output_data,\n","        'val_input_data': val_input_data,\n","        'val_output_data': val_output_data,\n","        'test_input_data': test_input_data,\n","        'test_output_data': test_output_data\n","    }\n","\n","    return tensors\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:45:06.215679Z","iopub.status.busy":"2024-04-30T04:45:06.215333Z","iopub.status.idle":"2024-04-30T04:45:06.221197Z","shell.execute_reply":"2024-04-30T04:45:06.220198Z","shell.execute_reply.started":"2024-04-30T04:45:06.215653Z"},"trusted":true},"outputs":[],"source":["# dict = {\n","# 'language' : 'hin',\n","# # 'dataset_path' : r'C:\\Users\\gragh\\OneDrive\\Desktop\\Codes\\CS6910 DL\\Assignment 3\\DataSet\\aksharantar_sampled',\n","# 'dataset_path' : '/kaggle/input/dl-ass3/aksharantar_sampled'\n","# }\n","# preprocessed_data = loadData(dict)\n","# tensors = create_tensor(preprocessed_data)\n","\n","# print('Input data : ', preprocessed_data['train_words'])\n","# print('Output data : ', preprocessed_data['train_translations'])\n","# print('Number of samples : ', len(preprocessed_data['train_words']))\n","\n","# print('Input data : ', preprocessed_data['val_words'])\n","# print('Output data : ', preprocessed_data['val_translations'])\n","# print('Number of val samples : ', len(preprocessed_data['val_words']))\n","\n","# print('Input data : ', preprocessed_data['test_words'])\n","# print('Output data : ', preprocessed_data['test_translations'])\n","# print('Number of test samples : ', len(preprocessed_data['test_words']))\n","\n","# print('Max incoder length : ', preprocessed_data['max_enc_len'])\n","# print('Max incoder length : ', preprocessed_data['max_enc_len'])\n","# print('Max length : ', preprocessed_data['max_len'])\n","\n","# print('Input index length', len(preprocessed_data['input_index']))\n","# print('Output index length', len(preprocessed_data['output_index']))\n","# print('Input index', preprocessed_data['input_index'])\n","# print('Output index', preprocessed_data['output_index'])\n","# print('Input index Rev', preprocessed_data['input_index_rev'])\n","# print('Output index Rev', preprocessed_data['output_index_rev'])\n","\n","# print('Input Data', tensors['input_data'].shape)\n","# print('Output Data', tensors['output_data'].shape)\n","# print('Input Data Val', tensors['val_input_data'].shape)\n","# print('Output Data Val', tensors['val_output_data'].shape)\n","# print('Input Data Test', tensors['test_input_data'].shape)\n","# print('Output Data Test', tensors['test_output_data'].shape)\n","\n","# print(tensors['input_data'][:,0])\n","# print(tensors['output_data'][:,0])"]},{"cell_type":"markdown","metadata":{},"source":["# ATTENTION"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:45:06.269009Z","iopub.status.busy":"2024-04-30T04:45:06.268414Z","iopub.status.idle":"2024-04-30T04:45:06.277431Z","shell.execute_reply":"2024-04-30T04:45:06.276419Z","shell.execute_reply.started":"2024-04-30T04:45:06.268981Z"},"trusted":true},"outputs":[],"source":["class Attention(nn.Module):\n","  \"\"\"\n","  This class implements an attention mechanism for a Seq2Seq model.\n","\n","  The attention mechanism allows the decoder to focus on relevant parts of the encoder output\n","  during the decoding process, improving the model's ability to translate sequences.\n","  \"\"\"\n","\n","  def __init__(self, hidden_size):\n","    \"\"\"\n","    Initializes the attention layer.\n","\n","    Args:\n","        hidden_size (int): The size of the hidden state vectors in the model.\n","    \"\"\"\n","    super(Attention, self).__init__()\n","    self.hidden_size = hidden_size  # Store the hidden size for calculations\n","\n","  def dot_score(self, hidden_state, encoder_states):\n","    \"\"\"\n","    Calculates the attention scores between the decoder hidden state and encoder outputs.\n","\n","    Args:\n","        hidden_state (torch.Tensor): The hidden state of the decoder at a specific time step.\n","        encoder_states (torch.Tensor): A tensor containing the encoder outputs for all time steps.\n","\n","    Returns:\n","        torch.Tensor: A tensor containing the attention scores for each encoder output.\n","    \"\"\"\n","    # Calculate the dot product between the decoder hidden state and each encoder output vector\n","    return torch.sum(hidden_state * encoder_states, dim=2)  # Summation over the feature dimension\n","\n","  def forward(self, hidden, encoder_outputs):\n","    \"\"\"\n","    Calculates the attention weights for a given decoder hidden state and encoder outputs.\n","\n","    Args:\n","        hidden (torch.Tensor): The hidden state of the decoder at a specific time step.\n","        encoder_outputs (torch.Tensor): A tensor containing the encoder outputs for all time steps.\n","\n","    Returns:\n","        torch.Tensor: A tensor containing the attention weights for each encoder output.\n","    \"\"\"\n","    # Calculate attention scores using dot product\n","    attn_scores = self.dot_score(hidden, encoder_outputs)\n","\n","    # Transpose the scores for softmax calculation (scores for each encoder output)\n","    attn_scores = attn_scores.t()\n","\n","    # Apply softmax to get normalized attention weights (sum to 1)\n","    attn_weights = F.softmax(attn_scores, dim=1)\n","\n","    # Unsqueeze to add a dimension for compatibility with decoder calculations\n","    return attn_weights.unsqueeze(1)\n"]},{"cell_type":"markdown","metadata":{},"source":["# ENCODER"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:45:06.332001Z","iopub.status.busy":"2024-04-30T04:45:06.331360Z","iopub.status.idle":"2024-04-30T04:45:06.344945Z","shell.execute_reply":"2024-04-30T04:45:06.343927Z","shell.execute_reply.started":"2024-04-30T04:45:06.331959Z"},"trusted":true},"outputs":[],"source":["class Encoder_Attention(nn.Module):\n","  \"\"\"\n","  This class implements the encoder part of a Seq2Seq model with attention.\n","\n","  The encoder takes a sequence of word indices as input and processes it to\n","  generate an encoded representation that captures the meaning of the sequence.\n","  \"\"\"\n","\n","  def __init__(self, params, preprocessed_data):\n","    \"\"\"\n","    Initializes the encoder.\n","\n","    Args:\n","        params (dict): A dictionary containing hyperparameters for the model.\n","        preprocessed_data (dict): A dictionary containing the preprocessed data.\n","    \"\"\"\n","    super(Encoder_Attention, self).__init__()\n","\n","    # Get hyperparameters\n","    self.cell_type = params['cell_type']  # Type of RNN cell (RNN, LSTM, or GRU)\n","    self.bi_directional = params['bi_dir']  # Whether to use a bidirectional RNN\n","    self.embedding_size = params['embedding_size']  # Dimensionality of word embeddings\n","    self.hidden_size = params['hidden_size']  # Dimensionality of hidden state\n","    self.dropout = nn.Dropout(params['dropout'])  # Dropout layer for regularization\n","\n","    # Embedding layer\n","    self.embedding = nn.Embedding(len(preprocessed_data['input_index']), self.embedding_size)\n","    # Look up an embedding vector for each word index in the input sequence\n","\n","    # Choose RNN cell based on type\n","    if self.cell_type == 'RNN':\n","      self.cell = nn.RNN(self.embedding_size, self.hidden_size, params['num_layers_enc'], dropout=params['dropout'], bidirectional=self.bi_directional)\n","    elif self.cell_type == 'LSTM':\n","      self.cell = nn.LSTM(self.embedding_size, self.hidden_size, params['num_layers_enc'], dropout=params['dropout'], bidirectional=self.bi_directional)\n","    elif self.cell_type == 'GRU':\n","      self.cell = nn.GRU(self.embedding_size, self.hidden_size, params['num_layers_enc'], dropout=params['dropout'], bidirectional=self.bi_directional)\n","    else:\n","      raise ValueError(\"Invalid type. Choose from 'RNN', 'LSTM', or 'GRU'.\")\n","\n","  def forward(self, x):\n","      \"\"\"\n","      Performs the forward pass through the encoder.\n","\n","      Args:\n","          x (torch.Tensor): A tensor containing a sequence of word indices.\n","\n","      Returns:\n","          tuple:\n","              - encoder_states (torch.Tensor): The encoded representation of the input sequence for all time steps.\n","              - hidden (torch.Tensor): The hidden state of the RNN at the last time step (if unidirectional) or a tuple of hidden states for both directions (if bidirectional).\n","              - cell (torch.Tensor): The cell state of the LSTM at the last time step (if LSTM is used). (Optional, only returned for LSTMs)\n","      \"\"\"\n","      embedding = self.dropout(self.embedding(x))\n","      if self.cell_type == 'LSTM':\n","          encoder_states, (hidden, cell) = self.cell(embedding)\n","          if self.bi_directional:\n","              encoder_states = encoder_states[:, :, :self.hidden_size] + encoder_states[:, : ,self.hidden_size:]\n","          return encoder_states, hidden, cell\n","      else:\n","          encoder_states, hidden = self.cell(embedding)\n","          if self.bi_directional:\n","              encoder_states = encoder_states[:, :, :self.hidden_size] + encoder_states[:, : ,self.hidden_size:]\n","          return encoder_states, hidden"]},{"cell_type":"markdown","metadata":{},"source":["# DECODER"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:45:06.401314Z","iopub.status.busy":"2024-04-30T04:45:06.400984Z","iopub.status.idle":"2024-04-30T04:45:06.419388Z","shell.execute_reply":"2024-04-30T04:45:06.418429Z","shell.execute_reply.started":"2024-04-30T04:45:06.401288Z"},"trusted":true},"outputs":[],"source":["class Decoder_Attention(nn.Module):\n","  \"\"\"\n","  This class implements the decoder part of a Seq2Seq model with attention.\n","\n","  The decoder takes an embedded target sequence (one word at a time) and the\n","  encoder outputs as input, and generates a sequence of predicted words.\n","  It uses attention to focus on relevant parts of the encoder outputs\n","  during the decoding process.\n","  \"\"\"\n","\n","  def __init__(self, params, preprocessed_data):\n","    \"\"\"\n","    Initializes the decoder.\n","\n","    Args:\n","        params (dict): A dictionary containing hyperparameters for the model.\n","        preprocessed_data (dict): A dictionary containing the preprocessed data.\n","    \"\"\"\n","    super(Decoder_Attention, self).__init__()\n","\n","    # Get hyperparameters\n","    self.cell_type = params['cell_type']  # Type of RNN cell (RNN, LSTM, or GRU)\n","    self.num_layers = params['num_layers_dec']  # Number of decoder layers\n","    self.dropout = nn.Dropout(params['dropout'])  # Dropout layer for regularization\n","    self.embedding_size = params['embedding_size']  # Dimensionality of word embeddings\n","\n","    # Embedding layer\n","    self.embedding = nn.Embedding(len(preprocessed_data['output_index']), params['embedding_size'])\n","    # Look up an embedding vector for each word index in the output sequence\n","\n","    # Choose RNN cell based on type\n","    if self.cell_type == 'RNN':\n","      self.cell = nn.RNN(params['embedding_size'], params['hidden_size'], self.num_layers, dropout=params['dropout'])\n","    elif self.cell_type == 'LSTM':\n","      self.cell = nn.LSTM(params['embedding_size'], params['hidden_size'], self.num_layers, dropout=params['dropout'])\n","    elif self.cell_type == 'GRU':\n","      self.cell = nn.GRU(params['embedding_size'], params['hidden_size'], self.num_layers, dropout=params['dropout'])\n","    else:\n","      raise ValueError(\"Invalid type. Choose from 'RNN', 'LSTM', or 'GRU'.\")\n","\n","    # Layers for combining decoder output and context vector\n","    self.concat = nn.Linear(params['hidden_size'] * 2, params['hidden_size'])  # Linear transformation for concatenation\n","    self.fc = nn.Linear(params['hidden_size'], len(preprocessed_data['output_index']))  # Final linear layer for prediction\n","\n","    # Attention layer\n","    self.attn = Attention(params['hidden_size'])  # Attention mechanism\n","\n","    # Softmax for probability distribution\n","    self.log_softmax = nn.LogSoftmax(dim = 1)\n","\n","  def forward(self, x, encoder_states, hidden, cell):\n","    \"\"\"\n","    Performs the forward pass through the decoder for a single time step.\n","\n","    Args:\n","        x (torch.Tensor): A tensor containing a single word index (input to decoder at this step).\n","        encoder_states (torch.Tensor): The encoded representation of the input sequence from the encoder.\n","        hidden (torch.Tensor): The hidden state of the decoder from the previous time step.\n","        cell (torch.Tensor): The cell state of the LSTM decoder from the previous time step (if LSTM is used).\n","\n","    Returns:\n","        tuple:\n","            - predictions (torch.Tensor): The log-softmax probabilities of the next predicted word.\n","            - hidden (torch.Tensor): The hidden state of the decoder for the current time step.\n","            - cell (torch.Tensor): The cell state of the LSTM decoder for the current time step (if LSTM is used).\n","            - attention_weights (torch.Tensor): The attention weights for the current time step.\n","    \"\"\"\n","    # Embed the input word\n","    embedding = self.dropout(self.embedding(x.unsqueeze(0)))\n","    # Pass the embedded word through the chosen cell\n","    if self.cell_type == 'LSTM':\n","        outputs, (hidden, cell) = self.cell(embedding, (hidden, cell))\n","        attention_weights = self.attn(outputs, encoder_states)\n","        context = attention_weights.bmm(encoder_states.transpose(0, 1))\n","        outputs = outputs.squeeze(0)\n","        context = context.squeeze(1)\n","        concat_input = torch.cat((outputs, context), 1)\n","        concat_output = torch.tanh(self.concat(concat_input))\n","        predictions = self.log_softmax(self.fc(concat_output))\n","        return predictions, hidden, cell, attention_weights.squeeze(1)\n","    else:\n","        outputs, (hidden) = self.cell(embedding, hidden)\n","        attention_weights = self.attn(outputs, encoder_states)\n","        context = attention_weights.bmm(encoder_states.transpose(0, 1))\n","        outputs = outputs.squeeze(0)\n","        context = context.squeeze(1)\n","        concat_input = torch.cat((outputs, context), 1)\n","        concat_output = torch.tanh(self.concat(concat_input))\n","        predictions = self.log_softmax(self.fc(concat_output))\n","        return predictions, hidden, attention_weights.squeeze(1)"]},{"cell_type":"markdown","metadata":{},"source":["# SEQ 2 SEQ"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:45:06.461599Z","iopub.status.busy":"2024-04-30T04:45:06.461280Z","iopub.status.idle":"2024-04-30T04:45:06.474062Z","shell.execute_reply":"2024-04-30T04:45:06.473131Z","shell.execute_reply.started":"2024-04-30T04:45:06.461566Z"},"trusted":true},"outputs":[],"source":["class Seq2Seq_Attention(nn.Module):\n","  \"\"\"\n","  This class implements a Seq2Seq model with attention mechanism.\n","\n","  The model takes an encoded source sequence and a target sequence as input,\n","  and generates a predicted target sequence using the decoder with attention.\n","  \"\"\"\n","\n","  def __init__(self, encoder, decoder, params, preprocessed_data):\n","    \"\"\"\n","    Initializes the Seq2Seq model.\n","\n","    Args:\n","        encoder (nn.Module): The encoder module of the model.\n","        decoder (nn.Module): The decoder module of the model.\n","        params (dict): A dictionary containing hyperparameters for the model.\n","        preprocessed_data (dict): A dictionary containing the preprocessed data.\n","    \"\"\"\n","    super(Seq2Seq_Attention, self).__init__()\n","\n","    # Get hyperparameters\n","    self.cell_type = params['cell_type']  # Type of RNN cell (RNN, LSTM, or GRU)\n","    self.encoder = encoder  # Encoder module\n","    self.decoder = decoder  # Decoder module\n","    self.num_layers_dec = params['num_layers_dec']  # Number of decoder layers\n","    self.output_index_len = len(preprocessed_data['output_index'])  # Vocabulary size for output language\n","    self.tfr = params['teacher_fr']  # Teacher forcing ratio\n","\n","  def forward(self, source, target):\n","    \"\"\"\n","    Performs the forward pass through the entire Seq2Seq model.\n","\n","    Args:\n","        source (torch.Tensor): A tensor containing the source sequence (encoder input).\n","        target (torch.Tensor): A tensor containing the target sequence (ground truth for training or prediction).\n","\n","    Returns:\n","        torch.Tensor: A tensor containing the predicted target sequence log-softmax probabilities.\n","    \"\"\"\n","\n","    # Get batch size and target sequence length\n","    batch_size, target_len = source.shape[1], target.shape[0]\n","\n","    # Start with the first word from the target sequence\n","    x = target[0, :]  # First element from each batch in the target sequence\n","\n","    # Initialize empty tensor to store predictions\n","    outputs = torch.zeros(target_len, batch_size, self.output_index_len).to(device)\n","\n","    # Get encoder outputs (encoded representation of the source sequence)\n","    if self.cell_type == 'LSTM':\n","      encoder_op, hidden, cell = self.encoder(source)\n","      # Truncate cell state to match decoder layer number\n","      cell = cell[:self.decoder.num_layers]\n","    else:\n","      encoder_op, hidden = self.encoder(source)\n","    # Truncate hidden state to match decoder layer number\n","    hidden = hidden[:self.decoder.num_layers]\n","\n","    # Iterate over the target sequence length (decoding process)\n","    for t in range(1, target_len):\n","      # Use LSTM cell state or hidden state depending on cell type\n","      if self.cell_type == 'LSTM':\n","        output, hidden, cell, _ = self.decoder(x, encoder_op, hidden, cell)\n","      else:\n","        output, hidden, _ = self.decoder(x, encoder_op, hidden, None)\n","\n","      # Store the predicted word probabilities and get the most likely word index\n","      outputs[t], best_guess = output, output.argmax(1)\n","\n","      # Teacher forcing: Choose predicted word or target word based on random probability\n","      x = best_guess if random.random() >= self.tfr else target[t]\n","\n","    # Return the tensor containing the predicted target sequence log-softmax probabilities\n","    return outputs\n"]},{"cell_type":"markdown","metadata":{},"source":["# GET OPTIMIZERS"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:45:06.534104Z","iopub.status.busy":"2024-04-30T04:45:06.533786Z","iopub.status.idle":"2024-04-30T04:45:06.543136Z","shell.execute_reply":"2024-04-30T04:45:06.542080Z","shell.execute_reply.started":"2024-04-30T04:45:06.534078Z"},"trusted":true},"outputs":[],"source":["def get_optim(model, params):\n","    \"\"\"\n","    This function creates an optimizer object based on the specified parameters.\n","\n","    Args:\n","        model (nn.Module): The Seq2Seq model instance.\n","        params (dict): A dictionary containing hyperparameters for the optimizer.\n","            - 'optimizer' (str): The name of the optimizer to use (e.g., 'sgd', 'adam', 'rmsprop', 'adagrad').\n","            - 'learning_rate' (float): The learning rate for the optimizer.\n","\n","    Returns:\n","        optim.Optimizer: An optimizer object for training the model.\n","    \"\"\"\n","\n","    optimizer_name = params['optimizer'].lower()  # Convert optimizer name to lowercase for case-insensitive matching\n","\n","    # Define the optimizer based on the specified name\n","    if optimizer_name == 'sgd':\n","        optimizer = optim.SGD(model.parameters(), lr=params['learning_rate'], momentum=0.9)\n","    elif optimizer_name == 'adam':\n","        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], betas=(0.9, 0.999), eps=1e-8)\n","    elif optimizer_name == 'rmsprop':\n","        optimizer = optim.RMSprop(model.parameters(), lr=params['learning_rate'], alpha=0.99, eps=1e-8)\n","    elif optimizer_name == 'adagrad':\n","        optimizer = optim.Adagrad(model.parameters(), lr=params['learning_rate'], lr_decay=0, weight_decay=0,\n","                                  initial_accumulator_value=0, eps=1e-10)\n","    else:\n","        raise ValueError(\"Invalid optimizer. Choose from 'sgd', 'adam', 'rmsprop', or 'adagrad'.\")\n","\n","    return optimizer\n"]},{"cell_type":"markdown","metadata":{},"source":["# GET TOTAL PARAMETERS"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:45:06.587334Z","iopub.status.busy":"2024-04-30T04:45:06.586970Z","iopub.status.idle":"2024-04-30T04:45:06.592610Z","shell.execute_reply":"2024-04-30T04:45:06.591632Z","shell.execute_reply.started":"2024-04-30T04:45:06.587307Z"},"trusted":true},"outputs":[],"source":["def get_total_parameters(model):\n","  \"\"\"\n","  This function calculates the total number of trainable parameters in a PyTorch model.\n","\n","  Args:\n","      model (nn.Module): The PyTorch model to analyze.\n","\n","  Returns:\n","      int: The total number of trainable parameters in the model.\n","  \"\"\"\n","\n","  # Filter only trainable parameters\n","  total_params = sum(param.numel() for param in model.parameters() if param.requires_grad)\n","\n","  return total_params\n"]},{"cell_type":"markdown","metadata":{},"source":["# BEAM SEARCH"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:45:06.659579Z","iopub.status.busy":"2024-04-30T04:45:06.659216Z","iopub.status.idle":"2024-04-30T04:45:06.677447Z","shell.execute_reply":"2024-04-30T04:45:06.676403Z","shell.execute_reply.started":"2024-04-30T04:45:06.659552Z"},"trusted":true},"outputs":[],"source":["def beam_search(model, word, preprocessed_data, params, bw = 1, lp = 0.6):\n","    \"\"\"\n","    This function performs beam search to generate a translated sequence for a given source word sequence.\n","\n","    Args:\n","        model (nn.Module): The Seq2Seq model instance.\n","        word (str): The source word sequence to translate.\n","        preprocessed_data (dict): A dictionary containing the preprocessed data.\n","        bw (int): Beam width for beam search.\n","        lp (float): Length penalty factor for beam search.\n","        ct (str): The type of RNN cell used in the model (LSTM or GRU).\n","\n","    Returns:\n","        str: The predicted translated word sequence.\n","    \"\"\"\n","    data = np.zeros((preprocessed_data['max_len']+1, 1), dtype=np.int32)\n","    for idx, char in enumerate(word):\n","        data[idx, 0] = preprocessed_data['input_index'][char]\n","    data[idx + 1, 0] = preprocessed_data['input_index'][preprocessed_data['EOS']]\n","    data = torch.tensor(data, dtype=torch.int32).to(device)\n","    with torch.no_grad():\n","        if params['cell_type'] == 'LSTM':\n","            outputs, hidden, cell = model.encoder(data)\n","            cell =  cell[:params['num_layers_dec']]\n","        else:\n","            outputs, hidden = model.encoder(data)\n","    hidden =  hidden[:params['num_layers_dec']]\n","    output_start = preprocessed_data['output_index'][preprocessed_data['SOS']]\n","    out_reshape = np.array(output_start).reshape(1,)\n","    hidden_par = hidden.unsqueeze(0)\n","    initial_sequence = torch.tensor(out_reshape).to(device)\n","    beam = [(0.0, initial_sequence, hidden_par)]\n","    for i in range(len(preprocessed_data['output_index'])):\n","        candidates = []\n","        for score, seq, hidden in beam:\n","            if seq[-1].item() == preprocessed_data['output_index'][preprocessed_data['EOS']]:\n","                candidates.append((score, seq, hidden))\n","                continue\n","            reshape_last = np.array(seq[-1].item()).reshape(1, )\n","            hdn = hidden.squeeze(0) \n","            x = torch.tensor(reshape_last).to(device)\n","            if params['cell_type'] == 'LSTM':\n","                output, hidden, cell, _ = model.decoder(x, outputs, hdn, cell)\n","            else:\n","                output, hidden, _ = model.decoder(x, outputs, hdn, None)\n","            topk_probs, topk_tokens = torch.topk(F.softmax(output, dim=1), k = bw)               \n","            for prob, token in zip(topk_probs[0], topk_tokens[0]):\n","                new_seq = torch.cat((seq, token.unsqueeze(0)), dim=0)\n","                candidate_score = score + torch.log(prob).item() / (((len(new_seq) - 1) / 5) ** lp)\n","                candidates.append((candidate_score, new_seq, hidden.unsqueeze(0)))\n","        beam = heapq.nlargest(bw, candidates, key=lambda x: x[0])\n","    _, best_sequence, _ = max(beam, key=lambda x: x[0]) \n","    prediction = ''.join([preprocessed_data['output_index_rev'][token.item()] for token in best_sequence[1:]])\n","    return prediction[:-1]\n"]},{"cell_type":"markdown","metadata":{},"source":["# TRAINING FUNCTION"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:45:06.720890Z","iopub.status.busy":"2024-04-30T04:45:06.720547Z","iopub.status.idle":"2024-04-30T04:45:06.742604Z","shell.execute_reply":"2024-04-30T04:45:06.741730Z","shell.execute_reply.started":"2024-04-30T04:45:06.720866Z"},"trusted":true},"outputs":[],"source":["def train(model, criterion, optimizer, preprocessed_data, tensors, params):\n","    \"\"\"\n","    This function trains the Seq2Seq model and performs validation.\n","\n","    Args:\n","        model (nn.Module): The Seq2Seq model instance.\n","        criterion (nn.Module): The loss function for training (e.g., nn.NLLLoss).\n","        optimizer (optim.Optimizer): The optimizer used for training (e.g., Adam).\n","        preprocessed_data (dict): A dictionary containing the preprocessed data.\n","        tensors (dict): A dictionary containing PyTorch tensors for training and validation data.\n","        params (dict): A dictionary containing hyperparameters for training and evaluation.\n","\n","    Returns:\n","        tuple:\n","            - model (nn.Module): The trained Seq2Seq model.\n","            - val_accuracy (float): Overall character-level accuracy on the validation set.\n","            - val_accuracy_beam (float): Overall word-level accuracy on the validation set using beam search.\n","    \"\"\"\n","    # splitting data in batches\n","    train_data, train_result = torch.split(tensors['input_data'], params['batch_size'], dim = 1), torch.split(tensors['output_data'], params['batch_size'], dim = 1)\n","    val_data, val_result = torch.split(tensors['val_input_data'], params['batch_size'], dim=1), torch.split(tensors['val_output_data'], params['batch_size'], dim=1)\n","    \n","    # performing epochs\n","    for epoch in range(params['num_epochs']):\n","        total_words = 0\n","        correct_pred = 0\n","        total_loss = 0\n","        model.train()\n","        # training the model\n","        with tqdm(total = len(train_data), desc = 'Training') as pbar:\n","            for i, (x, y) in enumerate(zip(train_data, train_result)):\n","                target, inp_data = y.to(device), x.to(device)\n","                optimizer.zero_grad()\n","                output = model(inp_data, target)\n","                target = target.reshape(-1)\n","                output = output.reshape(-1, output.shape[2])\n","                # adding padding mask to ignore 0 paddding while calculating accuracy\n","                pad_mask = (target != preprocessed_data['output_index'][preprocessed_data['PAD']])\n","                target = target[pad_mask]\n","                output = output[pad_mask]\n","                \n","                loss = criterion(output, target)\n","                loss.backward()\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","                optimizer.step()\n","                total_loss += loss.item()\n","                total_words += target.size(0)\n","                correct_pred += torch.sum(torch.argmax(output, dim=1) == target).item()\n","                pbar.update(1)\n","        train_accuracy = (correct_pred / total_words)*100\n","        train_loss = total_loss / len(train_data)\n","        # setting model in evaluation mode to validate on validation set\n","        model.eval()\n","        with torch.no_grad():\n","            val_total_loss = 0\n","            val_total_words = 0\n","            val_correct_pred = 0\n","            with tqdm(total = len(val_data), desc = 'Validation') as pbar:\n","                for x_val, y_val in zip(val_data, val_result):\n","                    target_val, inp_data_val = y_val.to(device), x_val.to(device)\n","                    output_val = model(inp_data_val, target_val)\n","                    target_val = target_val.reshape(-1)\n","                    output_val = output_val.reshape(-1, output_val.shape[2])\n","                    \n","                    pad_mask = (target_val != preprocessed_data['output_index'][preprocessed_data['PAD']])\n","                    target_val = target_val[pad_mask]\n","                    output_val = output_val[pad_mask]\n","                    \n","                    val_loss = criterion(output_val, target_val)\n","                    val_total_loss += val_loss.item()\n","                    val_total_words += target_val.size(0)\n","                    val_correct_pred += torch.sum(torch.argmax(output_val, dim=1) == target_val).item()\n","                    pbar.update(1)\n","            val_accuracy = (val_correct_pred / val_total_words) * 100\n","            val_loss = val_total_loss / len(val_data)\n","            \n","            # checking word level accuracy on validation set\n","            correct_pred = 0\n","            total_words = len(preprocessed_data['val_words'])\n","            with tqdm(total = total_words, desc = 'Beam_Validation') as pbar_:\n","                for word, translation in zip(preprocessed_data['val_words'], preprocessed_data['val_translations']):\n","                    ans = beam_search(model, word, preprocessed_data, params, params['beam_width'], params['length_penalty'])\n","                    if ans == translation[1:-1]:\n","                        correct_pred += 1\n","                    pbar_.update(1)\n","        val_accuracy_beam = (correct_pred / total_words) * 100\n","        \n","        # logging the results\n","        print(f'''Epoch : {epoch+1}\n","              Train Accuracy : {train_accuracy:.4f}, Train Loss : {train_loss:.4f}\n","              Validation Accuracy Char Level : {val_accuracy:.4f}, Validation Loss : {val_loss:.4f}\n","              Validation Accuracy Word Level : {val_accuracy_beam:.4f},  Correctly predicted : {correct_pred}/{total_words}''')\n","        if params['w_log']:\n","            wandb.log(\n","                    {\n","                        'epoch': epoch+1,\n","                        'training_loss' : train_loss,\n","                        'training_accuracy_char' : train_accuracy,\n","                        'validation_loss' : val_loss,\n","                        'validation_accuracy_char' : val_accuracy,\n","                        'validation_accuracy_word' : val_accuracy_beam,\n","                        'correctly_predicted' : correct_pred\n","                    }\n","                )\n","    return model, val_accuracy, val_accuracy_beam"]},{"cell_type":"markdown","metadata":{},"source":["# EVALUATING FUNCTION"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:45:08.345014Z","iopub.status.busy":"2024-04-30T04:45:08.344200Z","iopub.status.idle":"2024-04-30T04:45:08.355142Z","shell.execute_reply":"2024-04-30T04:45:08.354084Z","shell.execute_reply.started":"2024-04-30T04:45:08.344985Z"},"trusted":true},"outputs":[],"source":["def evaluate_model(trained_model, data_type, preprocessed_data, params, bw=1, lp=0.6):\n","  \"\"\"\n","  This function evaluates the Seq2Seq model on a specified data type (e.g., 'val', 'test').\n","\n","  Args:\n","      trained_model (nn.Module): The trained Seq2Seq model instance.\n","      data_type (str): The type of data to evaluate on (e.g., 'val', 'test').\n","      preprocessed_data (dict): A dictionary containing the preprocessed data.\n","      params (dict): A dictionary containing hyperparameters for evaluation (beam search).\n","          - 'bw' (int, optional): Beam width for beam search (default 1).\n","          - 'lp' (float, optional): Length penalty factor for beam search (default 0.6).\n","      cell_type (str): The type of RNN cell used in the model (extracted from params).\n","\n","  Returns:\n","      tuple:\n","          - words (list): List of source words (without start/end tokens).\n","          - translations (list): List of reference translations (without start/end tokens).\n","          - predictions (list): List of predicted translated sequences (without start/end tokens).\n","          - results (list): List of 'Yes' or 'No' indicating correct/incorrect predictions.\n","          - accuracy (float): Overall accuracy of the model on the data type.\n","          - correct_pred (int): Number of correctly predicted translations.\n","  \"\"\"\n","\n","  # Extract data indices based on data type (e.g., 'val_words', 'test_translations')\n","  data_words = data_type + '_words'\n","  data_translations = data_type + '_translations'\n","\n","  # Set the model to evaluation mode\n","  trained_model.eval()\n","\n","  # Initialize variables for tracking results\n","  correct_pred = 0\n","  words, translations, predictions, results = [], [], [], []\n","  total_words = len(preprocessed_data[data_words])\n","\n","  # Progress bar for iterating through data\n","  with tqdm(total=total_words, desc=data_type) as pbar:\n","    for word, translation in zip(preprocessed_data[data_words], preprocessed_data[data_translations]):\n","      # Perform beam search to get the predicted translation\n","      ans = beam_search(trained_model, word, preprocessed_data, params, bw, lp)\n","\n","      # Extract source and target sequences without start/end tokens\n","      words.append(word[:-1])\n","      translations.append(translation[1:-1])\n","      predictions.append(ans)\n","\n","      # Check if the prediction matches the reference translation\n","      if ans == translation[1:-1]:\n","        correct_pred += 1\n","        results.append('Yes')\n","      else:\n","        results.append('No')\n","\n","      # Update progress bar\n","      pbar.update(1)\n","\n","  # Calculate overall accuracy\n","  accuracy = (correct_pred / total_words) * 100\n","\n","  return words, translations, predictions, results, accuracy, correct_pred\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:45:08.849689Z","iopub.status.busy":"2024-04-30T04:45:08.849284Z","iopub.status.idle":"2024-04-30T04:45:08.861854Z","shell.execute_reply":"2024-04-30T04:45:08.860889Z","shell.execute_reply.started":"2024-04-30T04:45:08.849658Z"},"trusted":true},"outputs":[],"source":["def predict(model, word, preprocessed_data, params):\n","    \"\"\"\n","    This function generates a predicted translation for a given source word sequence.\n","\n","    Args:\n","        model (nn.Module): The trained Seq2Seq model instance.\n","        word (str): The source word sequence to translate.\n","        preprocessed_data (dict): A dictionary containing the preprocessed data.\n","        params (dict): A dictionary containing hyperparameters for the model.\n","            - 'cell_type' (str): The type of RNN cell used in the model.\n","\n","    Returns:\n","        str: The predicted translated word sequence.\n","    \"\"\"\n","\n","    # Create a zero-filled data tensor with extra row for end-of-sequence (EOS) token\n","    data = np.zeros((preprocessed_data['max_len'] + 1, 1), dtype=int)\n","    pred = ''  # Initialize an empty string to store the predicted translation\n","\n","    # Encode the source word sequence (one word at a time)\n","    for t, char in enumerate(word):\n","        data[t, 0] = preprocessed_data['input_index'][char]\n","    data[(t + 1), 0] = preprocessed_data['input_index'][preprocessed_data['EOS']]  # Add EOS token\n","    data = torch.tensor(data, dtype=torch.int64).to(device)\n","\n","  # Disable gradient calculation for efficiency during prediction\n","    with torch.no_grad():\n","    # Get the hidden state(s) from the encoder\n","        if params['cell_type'] == 'LSTM':\n","            outputs, hidden, cell = model.encoder(data)\n","            cell =  cell[:params['num_layers_dec']]\n","        else:\n","            outputs, hidden = model.encoder(data)\n","    hidden =  hidden[:params['num_layers_dec']]\n","    # Start token (SOS) for the decoder\n","    x = torch.tensor([preprocessed_data['output_index'][preprocessed_data['SOS']]]).to(device)\n","    attentions = torch.zeros(preprocessed_data['max_len'] + 1, 1, preprocessed_data['max_len'] + 1)\n","    \n","    # Greedy search for predicted translation\n","    for t in range(1, len(preprocessed_data['output_index'])):\n","        if params['cell_type'] == 'LSTM':\n","            output, hidden, cell, attn = model.decoder(x, outputs, hidden, cell)\n","        else:\n","            output, hidden, attn = model.decoder(x, outputs, hidden, None)\n","        \n","        # Convert the decoder output to the predicted character\n","        character = preprocessed_data['output_index_rev'][output.argmax(1).item()]\n","        attentions[t] = attn\n","        if character != preprocessed_data['EOS']:\n","            pred = pred + character\n","        else:\n","            break\n","        \n","        # Use the predicted character as the next input to the decoder\n","        x = torch.tensor([output.argmax(1)]).to(device)        \n","    return pred, attentions[:t+1]"]},{"cell_type":"markdown","metadata":{},"source":["# LOG RESULTS"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:45:08.914829Z","iopub.status.busy":"2024-04-30T04:45:08.914497Z","iopub.status.idle":"2024-04-30T04:45:08.921523Z","shell.execute_reply":"2024-04-30T04:45:08.920513Z","shell.execute_reply.started":"2024-04-30T04:45:08.914802Z"},"trusted":true},"outputs":[],"source":["def store_results(data_type, words, translations, predictions, results):\n","  \"\"\"\n","  This function saves the evaluation results to a CSV file.\n","\n","  Args:\n","      data_type (str): The type of data used for evaluation (e.g., 'val', 'test').\n","      words (list): List of source words (without start/end tokens).\n","      translations (list): List of reference translations (without start/end tokens).\n","      predictions (list): List of predicted translated sequences (without start/end tokens).\n","      results (list): List of 'Yes' or 'No' indicating correct/incorrect predictions.\n","  \"\"\"\n","\n","  # Create a dictionary to store the results in a structured format\n","  log = {\n","      'Word': words,\n","      'Translation': translations,\n","      'Prediction': predictions,\n","      'Result': results\n","  }\n","\n","  # Construct the file path for the CSV file\n","  path = '/kaggle/working/predictions_attention.csv'\n","\n","  # Create a Pandas DataFrame from the dictionary\n","  data_frame = pd.DataFrame(log)\n","\n","  # Save the DataFrame to a CSV file (header=True includes column names, index=False excludes row index)\n","  data_frame.to_csv(path, header=True, index=False)\n","\n","  pd.DataFrame(log)"]},{"cell_type":"markdown","metadata":{},"source":["# Question 5"]},{"cell_type":"markdown","metadata":{},"source":["## HYPERPARAMETERS"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:45:09.031155Z","iopub.status.busy":"2024-04-30T04:45:09.030758Z","iopub.status.idle":"2024-04-30T04:45:09.037180Z","shell.execute_reply":"2024-04-30T04:45:09.035975Z","shell.execute_reply.started":"2024-04-30T04:45:09.031118Z"},"trusted":true},"outputs":[],"source":["params = {\n","#     'dataset_path' : r'C:\\Users\\gragh\\OneDrive\\Desktop\\Codes\\CS6910 DL\\Assignment 3\\DataSet\\aksharantar_sampled',\n","    'language' : 'hin',\n","    'dataset_path' : '/kaggle/input/dl-ass3/aksharantar_sampled',\n","    'embedding_size': 256,\n","    'hidden_size': 512,\n","    'num_layers_enc': 3,\n","    'num_layers_dec': 3,\n","    'cell_type': 'LSTM',\n","    'dropout': 0.3,\n","    'optimizer' : 'adagrad',\n","    'learning_rate': 0.01,\n","    'batch_size': 32,\n","    'num_epochs': 1,\n","    'teacher_fr' : 0.7,\n","    'length_penalty' : 0.6,\n","    'beam_width': 1,\n","    'bi_dir' : True,\n","    'w_log' : 0\n","}"]},{"cell_type":"markdown","metadata":{},"source":["## TRAINING MODEL"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:45:09.077809Z","iopub.status.busy":"2024-04-30T04:45:09.076976Z","iopub.status.idle":"2024-04-30T04:48:57.793096Z","shell.execute_reply":"2024-04-30T04:48:57.791954Z","shell.execute_reply.started":"2024-04-30T04:45:09.077778Z"},"trusted":true},"outputs":[],"source":["# pre precessing data and getting tensor representations\n","preprocessed_data = loadData(params)\n","tensors = create_tensor(preprocessed_data)\n","\n","# defining Encoder, Decoder and Model\n","encoder = Encoder_Attention(params, preprocessed_data).to(device)\n","decoder = Decoder_Attention(params, preprocessed_data).to(device)\n","model = Seq2Seq_Attention(encoder, decoder, params, preprocessed_data).to(device)  \n","# print(model)\n","\n","# defining Loss function and Optimizer\n","criterion = nn.CrossEntropyLoss(ignore_index = 0)\n","optimizer = get_optim(model,params)\n","# print(optimizer)\n","\n","# Print total number of parameters in the model\n","# total_parameters = get_total_parameters(model)\n","# print(f'Total Trainable Parameters: {total_parameters}')\n","\n","# logging to wandb\n","if params['w_log']:\n","    wandb.init(project = 'DL-Assignment-3')\n","    wandb.run.name = (\n","        'check_c:' + params['cell_type'] +\n","        '_e:' + str(params['num_epochs']) +\n","        '_es:' + str(params['embedding_size']) +\n","        '_hs:' + str(params['hidden_size']) +\n","        '_nle:' + str(params['num_layers_enc']) +\n","        '_nld:' + str(params['num_layers_dec']) +\n","        '_o:' + params['optimizer'] +\n","        '_lr:' + str(params['learning_rate']) +\n","        '_bs:' + str(params['batch_size']) +\n","        '_tf:' + str(params['teacher_fr']) +\n","        '_lp:' + str(params['length_penalty']) +\n","        '_b:' + str(params['bi_dir']) +\n","        '_bw:' + str(params['beam_width'])\n","    )\n","# training the model\n","trained_model, _, _ = train(model, criterion, optimizer, preprocessed_data, tensors, params)\n","if params['w_log']:\n","    wandb.finish()"]},{"cell_type":"markdown","metadata":{},"source":["# EVALUATE MODEL"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:48:57.795678Z","iopub.status.busy":"2024-04-30T04:48:57.795066Z","iopub.status.idle":"2024-04-30T04:48:58.743637Z","shell.execute_reply":"2024-04-30T04:48:58.742666Z","shell.execute_reply.started":"2024-04-30T04:48:57.795642Z"},"trusted":true},"outputs":[],"source":["words = ['raghav', 'iit', 'madras', 'nirav', 'nishchal', 'nishant', 'neymar', 'neha', 'harsh', 'rahul', 'rohit', 'hahahahaha', 'ohohohoh']\n","print('################################## using predict function ############################################################')\n","for w in words:\n","    output_sequence, _ = predict(trained_model, w, preprocessed_data, params)\n","    print(w,'->',output_sequence)\n","for w in preprocessed_data['val_words'][:10]:\n","    output_sequence, _ = predict(trained_model, w[:-1], preprocessed_data, params)\n","    print(w,'->',output_sequence)\n","print('####################################### using beam ###################################################################')\n","for w in words:\n","    output_sequence = beam_search(trained_model, w, preprocessed_data, params, params['beam_width'], params['length_penalty'])\n","    print(w,'->',output_sequence)\n","for w in preprocessed_data['val_words'][:10]:\n","    output_sequence = beam_search(trained_model, w, preprocessed_data, params, params['beam_width'], params['length_penalty'])\n","    print(w,'->',output_sequence)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T04:48:58.745198Z","iopub.status.busy":"2024-04-30T04:48:58.744888Z","iopub.status.idle":"2024-04-30T04:51:36.235923Z","shell.execute_reply":"2024-04-30T04:51:36.234942Z","shell.execute_reply.started":"2024-04-30T04:48:58.745171Z"},"trusted":true},"outputs":[],"source":["# Evaluating model for word level accuracy\n","\n","words_test, translations_test, predictions_test, results_test, accuracy_test_word_level, correct_pred_test = evaluate_model(trained_model, 'test', preprocessed_data, params, params['beam_width'], params['length_penalty'])\n","print(f'Test Accuracy Word Level : {accuracy_test_word_level}, Correctly Predicted : {correct_pred_test}')\n","store_results('test', words_test, translations_test, predictions_test, results_test)\n","\n","# words_train, translations_train, predictions_train, results_train, accuracy_train_word_level, correct_pred_train = evaluate_model(trained_model, 'train', preprocessed_data, params, params['beam_width'], params['length_penalty'])\n","# print(f'Train Accuracy Word Level : {accuracy_train_word_level}, Correctly Predicted : {correct_pred_train}')\n","# store_results('train', words_train, translations_train, predictions_train, results_train)\n","\n","# words_val, translations_val, predictions_val, results_val, accuracy_val_word_level, correct_pred_val = evaluate_model(trained_model, 'val', preprocessed_data, params, params['beam_width'], params['length_penalty'])\n","# print(f'Validation Accuracy Word Level : {accuracy_val_word_level}, Correctly Predicted : {correct_pred_val}')\n","# store_results('val', words_val, translations_val, predictions_val, results_val)"]},{"cell_type":"markdown","metadata":{},"source":["# LOG PREDICTIONS GRID"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T05:20:07.662687Z","iopub.status.busy":"2024-04-30T05:20:07.662317Z","iopub.status.idle":"2024-04-30T05:20:07.667986Z","shell.execute_reply":"2024-04-30T05:20:07.667160Z","shell.execute_reply.started":"2024-04-30T05:20:07.662652Z"},"trusted":true},"outputs":[],"source":["def csv_to_list(filename, num_rows=21):\n","  data_list = []\n","  with open(filename, 'r') as csvfile:\n","    reader = csv.reader(csvfile)\n","    for row in reader:\n","      if len(data_list) >= num_rows:\n","        break\n","      data_list.append(row)\n","  return data_list"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-30T05:20:08.454558Z","iopub.status.busy":"2024-04-30T05:20:08.453842Z","iopub.status.idle":"2024-04-30T05:20:32.391538Z","shell.execute_reply":"2024-04-30T05:20:32.390650Z","shell.execute_reply.started":"2024-04-30T05:20:08.454526Z"},"trusted":true},"outputs":[],"source":["data_list = csv_to_list('/kaggle/working/predictions_attention.csv')\n","columns = ['Word', 'Translation', 'Prediction', 'Result (Y/N)']\n","table = wandb.Table(columns=columns, data=data_list[1:])\n","wandb.init(project='DL-Assignment-3', name = 'Predictions Attention Grid')\n","wandb.log({'Predictions Attention Grid': table})\n","wandb.finish()"]},{"cell_type":"markdown","metadata":{},"source":["# ATTENTION HEATMAPS"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T19:57:36.162292Z","iopub.status.busy":"2024-04-29T19:57:36.160795Z","iopub.status.idle":"2024-04-29T19:57:36.175343Z","shell.execute_reply":"2024-04-29T19:57:36.174259Z","shell.execute_reply.started":"2024-04-29T19:57:36.162244Z"},"trusted":true},"outputs":[],"source":["def plot_attention_grid(model, w_log = 0):\n","  # getting 10 sample random pairs of words and translation from test data\n","  random_pairs = random.sample(list(zip(preprocessed_data['test_words'], preprocessed_data['test_translations'])), 10)\n","  inputs, outputs, attentions = [], [], []\n","  for i, (word_and_eos, translation_and_eos) in enumerate(random_pairs):\n","    word = word_and_eos[:-1]\n","    translation = translation_and_eos[:-1]\n","    output, attention = predict(model, word, preprocessed_data, params)\n","    attention = attention[1:, :, :(len(word))]\n","    inputs.append(word)\n","    outputs.append(' ' + output)  # Add space before predicted translation for better readability\n","    attentions.append(attention)\n","\n","  fig, axes = plt.subplots(4, 3, figsize=(15, 15))\n","  fig.suptitle('Attention Matrix Grid', fontsize=14)\n","\n","  for i in range(len(inputs)):\n","    word = inputs[i]  # Get the input word sequence\n","    translation = outputs[i]  # Get the corresponding translated sequence\n","    attention = attentions[i][:len(translation), :len(word)].squeeze(1).detach().numpy()  # Extract and reshape attention matrix\n","\n","    ax = axes.flat[i]\n","\n","    sns.heatmap(attention, cmap='YlGnBu', ax=ax)\n","    ax.set_xticks(np.arange(len(word)))  # X-axis ticks for input words\n","    ax.set_xticklabels(word, size=8)  # X-axis labels with word text\n","    ax.set_yticks(np.arange(len(translation)))  # Y-axis ticks for translated words\n","    hindi_font = FontProperties(fname='/kaggle/input/wordcloud-hindi-font/Nirmala.ttf')  # Assuming Hindi font path\n","    ax.set_yticklabels(translation, size=8, fontproperties=hindi_font)  # Y-axis labels with translated text (using Hindi font)\n","    ax.set_xlabel('Input Sequence', fontsize=10)\n","    ax.set_ylabel('Output Sequence', fontsize=10)\n","\n","    ax.grid(color='lightgray', linestyle='-', linewidth=1)\n","\n","  for ax in axes.flat[10:]:\n","    ax.axis('off')\n","\n","  fig.tight_layout()\n","  if w_log:\n","    wandb.init(project='DL-Assignment-3', name = 'Attention Matrix Grid')\n","    wandb.log({'Attention Matrix': wandb.Image(plt)})\n","    wandb.finish()\n","  plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T19:57:36.635010Z","iopub.status.busy":"2024-04-29T19:57:36.634619Z","iopub.status.idle":"2024-04-29T19:57:44.419021Z","shell.execute_reply":"2024-04-29T19:57:44.417625Z","shell.execute_reply.started":"2024-04-29T19:57:36.634980Z"},"trusted":true},"outputs":[],"source":["# Plot the attention grid for the sampled word-translation pairs\n","plot_attention_grid(trained_model, w_log = 0)"]},{"cell_type":"markdown","metadata":{},"source":["# TUNING HYPERPARAMETERS"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sweep_config = {\n","            'name': 'sweep_attn 1 : random',\n","            'method': 'random',\n","            'metric': { 'goal': 'maximize','name': 'Accuracy'},\n","            'parameters': \n","                {\n","                    'num_epochs': {'values': [10]},\n","                    'cell_type': {'values': ['RNN', 'LSTM', 'GRU']},\n","                    'embedding_size': {'values': [128, 256, 512]},\n","                    'hidden_size': {'values': [128, 256, 512]},\n","                    'num_layers': {'values': [1, 2, 3]},\n","                    'dropout': {'values': [0.3, 0.5, 0.7]},\n","                    'optimizer' : {'values' : ['adam', 'sgd', 'rmsprop', 'adagrad']},\n","                    'learning_rate': {'values': [0.001, 0.005, 0.01, 0.1]},\n","                    'batch_size': {'values': [32, 64]},\n","                    'teacher_fr' : {'values': [0.3, 0.5, 0.7]},\n","                    'length_penalty' : {'values': [0.4, 0.5, 0.6]},\n","                    'bi_dir' : {'values': [True, False]},\n","                    'beam_width': {'values': [1, 2, 3, 4, 5]}\n","                }\n","            }"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# sweep_config = {\n","#             'name': 'sweep_attn 2 : bayes',\n","#             'method': 'bayes',\n","#             'metric': { 'goal': 'maximize','name': 'Accuracy'},\n","#             'parameters': \n","#                 {\n","#                     'num_epochs': {'values': [10]},\n","#                     'cell_type': {'values': ['LSTM', 'GRU']},\n","#                     'embedding_size': {'values': [256]},\n","#                     'hidden_size': {'values': [256, 512]},\n","#                     'num_layers': {'values': [1, 2, 3]},\n","#                     'dropout': {'values': [0.3]},\n","#                     'optimizer' : {'values' : ['adam', 'adagrad']},\n","#                     'learning_rate': {'values': [0.001, 0.01]},\n","#                     'batch_size': {'values': [32]},\n","#                     'teacher_fr' : {'values': [0.7]},\n","#                     'length_penalty' : {'values': [0.6]},\n","#                     'bi_dir' : {'values': [True]},\n","#                     'beam_width': {'values': [1, 2, 3]}\n","#                 }\n","#             }"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# sweep function\n","def run_sweep():\n","    init = wandb.init(project = 'DL-Assignment-3')\n","    config = init.config\n","    params = {\n","        'language' : 'hin',\n","        'dataset_path' : '/kaggle/input/dl-ass3/aksharantar_sampled',\n","        'num_epochs': config.num_epochs,\n","        'cell_type': config.cell_type,\n","        'embedding_size': config.embedding_size,\n","        'hidden_size': config.hidden_size,\n","        'num_layers_enc': config.num_layers,\n","        'num_layers_dec': config.num_layers,\n","        'dropout': config.dropout,\n","        'optimizer' : config.optimizer,\n","        'learning_rate': config.learning_rate,\n","        'batch_size': config.batch_size,\n","        'teacher_fr' : config.teacher_fr,\n","        'length_penalty' : config.length_penalty,\n","        'bi_dir' : config.bi_dir,\n","        'beam_width' : config.beam_width,\n","        'w_log' : 1\n","    }\n","    \n","    wandb.run.name = (\n","        'Q5_c:' + params['cell_type'] +\n","        '_e' + str(params['num_epochs']) +\n","        '_es:' + str(params['embedding_size']) +\n","        '_hs:' + str(params['hidden_size']) +\n","        '_nle:' + str(params['num_layers_enc']) +\n","        '_nld:' + str(params['num_layers_dec']) +\n","        '_o:' + params['optimizer'] +\n","        '_lr:' + str(params['learning_rate']) +\n","        '_bs:' + str(params['batch_size']) +\n","        '_tf:' + str(params['teacher_fr']) +\n","        '_lp:' + str(params['length_penalty']) +\n","        '_b:' + str(params['bi_dir']) +\n","        '_bw:' + str(params['beam_width'])\n","    )\n","    preprocessed_data = loadData(params)\n","    tensors = create_tensor(preprocessed_data)\n","    \n","    encoder = Encoder_Attention(params, preprocessed_data).to(device)\n","    decoder = Decoder_Attention(params, preprocessed_data).to(device)\n","    model = Seq2Seq_Attention(encoder, decoder, params, preprocessed_data).to(device) \n","    \n","    criterion = nn.CrossEntropyLoss(ignore_index = 0)\n","    optimizer = get_optim(model,params)\n","    _, _, v_acc_beam = train(model, criterion, optimizer, preprocessed_data, tensors, params)\n","    wandb.log({'Accuracy': v_acc_beam})"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # running sweeps\n","# sweep_id = wandb.sweep(sweep_config, project='DL-Assignment-3')\n","# wandb.agent(sweep_id, run_sweep, count = 20)\n","# wandb.finish()"]},{"cell_type":"markdown","metadata":{},"source":["# LOG COMPARISON TABLE"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def log_comparison(w_log = 0):\n","    table1 = pd.read_csv('predictions_vanilla.csv')\n","    table2 = pd.read_csv('predictions_attention.csv')\n","\n","    merged_tables = pd.merge(table1, table2, on='Word', suffixes=('_vanilla', '_attention'))\n","    filtered_table = merged_tables[(merged_tables['Result_vanilla'] == 'No') & (merged_tables['Result_attention'] == 'Yes')]\n","    filtered_table['Translation'] = filtered_table['Translation_vanilla']\n","    filtered_table = filtered_table[['Word', 'Translation', 'Prediction_vanilla', 'Prediction_attention', 'Result_attention']]\n","    print(filtered_table)\n","    if w_log:\n","        wandb.init(project='DL-Assignment-3', name='Predictions_Comparison')\n","        wandb.log({'Vanilla_vs_Attention': wandb.Table(dataframe = filtered_table)})\n","        wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# log_comparison(1)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":883208,"sourceId":1508471,"sourceType":"datasetVersion"},{"datasetId":4868284,"sourceId":8213932,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
