{"cells":[{"cell_type":"markdown","metadata":{},"source":["# IMPORTS"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T21:58:19.451568Z","iopub.status.busy":"2024-04-25T21:58:19.450964Z","iopub.status.idle":"2024-04-25T21:58:23.633214Z","shell.execute_reply":"2024-04-25T21:58:23.632474Z","shell.execute_reply.started":"2024-04-25T21:58:19.451528Z"},"trusted":true},"outputs":[],"source":["import csv\n","import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import heapq\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","from torch.nn.utils import clip_grad_norm_\n","import random\n","import wandb\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T21:58:23.634899Z","iopub.status.busy":"2024-04-25T21:58:23.634614Z","iopub.status.idle":"2024-04-25T21:58:23.666201Z","shell.execute_reply":"2024-04-25T21:58:23.665217Z","shell.execute_reply.started":"2024-04-25T21:58:23.634873Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T21:58:23.667500Z","iopub.status.busy":"2024-04-25T21:58:23.667228Z","iopub.status.idle":"2024-04-25T21:58:26.554709Z","shell.execute_reply":"2024-04-25T21:58:26.553556Z","shell.execute_reply.started":"2024-04-25T21:58:23.667477Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}],"source":["!wandb login 3c81526a5ec348850a4c9d0f852f6631959307ed"]},{"cell_type":"markdown","metadata":{},"source":["# PREPROCESSING"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T22:01:17.071501Z","iopub.status.busy":"2024-04-25T22:01:17.071129Z","iopub.status.idle":"2024-04-25T22:01:17.089738Z","shell.execute_reply":"2024-04-25T22:01:17.088845Z","shell.execute_reply.started":"2024-04-25T22:01:17.071470Z"},"trusted":true},"outputs":[],"source":["def loadData(params):\n","    language = params['language']\n","    dataset_path = params['dataset_path']\n","    train_path = os.path.join(dataset_path, language, language + '_train.csv')\n","    val_path = os.path.join(dataset_path, language, language + '_valid.csv')\n","    test_path = os.path.join(dataset_path, language, language + '_test.csv')\n","    train_data = csv.reader(open(train_path,encoding='utf8'))\n","    val_data = csv.reader(open(val_path,encoding='utf8'))\n","    test_data = csv.reader(open(test_path,encoding='utf8'))\n","    train_words , train_translations = [], []\n","    val_words , val_translations = [], []\n","    test_words , test_translations = [], []\n","    pad, start, end ='', '^', '$'\n","    \n","    for pair in train_data:\n","        train_words.append(pair[0] + end)\n","        train_translations.append(start + pair[1] + end)\n","    for pair in val_data:\n","        val_words.append(pair[0] + end)\n","        val_translations.append(start + pair[1] + end)\n","    for pair in test_data:\n","        test_words.append(pair[0] + end)\n","        test_translations.append(start + pair[1] + end)\n","    \n","    train_words , train_translations = np.array(train_words), np.array(train_translations)\n","    val_words , val_translations = np.array(val_words), np.array(val_translations)\n","    test_words , test_translations = np.array(test_words), np.array(test_translations)\n","    input_vocab = set()\n","    output_vocab = set()\n","    \n","    for w in train_words:\n","        for c in w:\n","            input_vocab.add(c)\n","    for w in val_words:\n","        for c in w:\n","            input_vocab.add(c)\n","    for w in test_words:\n","        for c in w:\n","            input_vocab.add(c)\n","            \n","    for w in train_translations:\n","        for c in w:\n","            output_vocab.add(c)\n","    for w in val_translations:\n","        for c in w:\n","            output_vocab.add(c)\n","    for w in test_translations:\n","        for c in w:\n","            output_vocab.add(c)\n","    \n","    input_vocab.remove(end)\n","    output_vocab.remove(start)\n","    output_vocab.remove(end)  \n","    input_vocab, output_vocab = [pad, start, end] + list(sorted(input_vocab)), [pad, start, end] + list(sorted(output_vocab))\n","            \n","    input_index = {char: idx for idx, char in enumerate(input_vocab)}\n","    output_index = {char: idx for idx, char in enumerate(output_vocab)}\n","    # output_index =  dict([(char, idx) for idx, char in enumerate(output_vocab)])\n","    input_index_rev = {idx: char for char, idx in input_index.items()}\n","    output_index_rev = {idx: char for char, idx in output_index.items()}\n","    \n","    max_enc_len = max([len(word) for word in np.hstack((train_words, test_words, val_words))])\n","    max_dec_len = max([len(word) for word in np.hstack((train_translations, val_translations, test_translations))])\n","    max_len = max(max_enc_len, max_dec_len)\n","        \n","    preprocessed_data = {\n","        'SOS' : start,\n","        'EOS' : end,\n","        'PAD' : pad,\n","        'train_words' : train_words,\n","        'train_translations' : train_translations,\n","        'val_words' : val_words,\n","        'val_translations' : val_translations,\n","        'test_words' : test_words,\n","        'test_translations' : test_translations,\n","        'max_enc_len' : max_enc_len,\n","        'max_dec_len' : max_dec_len,\n","        'max_len' : max_len,\n","        'input_index' : input_index,\n","        'output_index' : output_index,\n","        'input_index_rev' : input_index_rev,\n","        'output_index_rev' : output_index_rev\n","    }\n","    return preprocessed_data"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T22:01:18.037774Z","iopub.status.busy":"2024-04-25T22:01:18.036950Z","iopub.status.idle":"2024-04-25T22:01:18.052123Z","shell.execute_reply":"2024-04-25T22:01:18.051229Z","shell.execute_reply.started":"2024-04-25T22:01:18.037740Z"},"trusted":true},"outputs":[],"source":["def create_tensor(preprocessed_data):\n","    input_data = np.zeros((preprocessed_data['max_len'],len(preprocessed_data['train_words'])), dtype = 'int64')\n","    output_data = np.zeros((preprocessed_data['max_len'],len(preprocessed_data['train_words'])), dtype = 'int64')\n","    \n","    val_input_data = np.zeros((preprocessed_data['max_len'],len(preprocessed_data['val_words'])), dtype = 'int64')\n","    val_output_data = np.zeros((preprocessed_data['max_len'],len(preprocessed_data['val_words'])), dtype = 'int64')\n","    \n","    test_input_data = np.zeros((preprocessed_data['max_len'],len(preprocessed_data['test_words'])), dtype = 'int64')\n","    test_output_data = np.zeros((preprocessed_data['max_len'],len(preprocessed_data['test_words'])), dtype = 'int64')\n","    \n","    for idx, (w, t) in enumerate(zip(preprocessed_data['train_words'], preprocessed_data['train_translations'])):\n","        for i, char in enumerate(w):\n","            input_data[i, idx] = preprocessed_data['input_index'][char]\n","        for i, char in enumerate(t):\n","            output_data[i, idx] = preprocessed_data['output_index'][char]\n","        \n","    for idx, (w, t) in enumerate(zip(preprocessed_data['val_words'], preprocessed_data['val_translations'])):\n","        for i, char in enumerate(w):\n","            val_input_data[i, idx] = preprocessed_data['input_index'][char]\n","        for i, char in enumerate(t):\n","            val_output_data[i, idx] = preprocessed_data['output_index'][char]\n","    \n","    for idx, (w, t) in enumerate(zip(preprocessed_data['test_words'], preprocessed_data['test_translations'])):\n","        for i, char in enumerate(w):\n","            test_input_data[i, idx] = preprocessed_data['input_index'][char]\n","        for i, char in enumerate(t):\n","            test_output_data[i, idx] = preprocessed_data['output_index'][char]\n","    \n","    input_data, output_data = torch.tensor(input_data,dtype = torch.int64), torch.tensor(output_data, dtype = torch.int64)\n","    val_input_data, val_output_data = torch.tensor(val_input_data,dtype = torch.int64), torch.tensor(val_output_data, dtype = torch.int64)\n","    test_input_data, test_output_data = torch.tensor(test_input_data,dtype = torch.int64), torch.tensor(test_output_data, dtype = torch.int64)\n","    \n","    tensors = {\n","        'input_data' : input_data,\n","        'output_data' : output_data,\n","        'val_input_data' : val_input_data,\n","        'val_output_data' : val_output_data, \n","        'test_input_data' : test_input_data,\n","        'test_output_data' : test_output_data\n","    }\n","    return tensors"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T22:01:19.269098Z","iopub.status.busy":"2024-04-25T22:01:19.268499Z","iopub.status.idle":"2024-04-25T22:01:20.278858Z","shell.execute_reply":"2024-04-25T22:01:20.277841Z","shell.execute_reply.started":"2024-04-25T22:01:19.269068Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Input data :  ['shastragaar$' 'bindhya$' 'kirankant$' ... 'asahmaton$' 'sulgaayin$'\n"," 'anchuthengu$']\n","Output data :  ['^शस्त्रागार$' '^बिन्द्या$' '^किरणकांत$' ... '^असहमतों$' '^सुलगायीं$'\n"," '^अंचुतेंगु$']\n","Number of samples :  51200\n","Input data :  ['jaisawal$' 'bajai$' 'sanghthan$' ... 'ekamreshwar$' 'bluetooth$'\n"," 'govindram$']\n","Output data :  ['^जयसवाल$' '^बजाई$' '^संघठन$' ... '^एकाम्रेश्वर$' '^ब्ल्यूटूथ$'\n"," '^गोविंद्राम$']\n","Number of val samples :  4096\n","Input data :  ['thermax$' 'sikhaaega$' 'learn$' ... 'khaatootolaa$' 'shivastava$'\n"," 'preranapuree$']\n","Output data :  ['^थरमैक्स$' '^सिखाएगा$' '^लर्न$' ... '^खातूटोला$' '^शिवास्तव$'\n"," '^प्रेरणापुरी$']\n","Number of test samples :  4096\n","Max incoder length :  27\n","Max incoder length :  27\n","Max length :  27\n","Input index length 29\n","Output index length 68\n","Input index {'': 0, '^': 1, '$': 2, 'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'j': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'p': 18, 'q': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28}\n","Output index {'': 0, '^': 1, '$': 2, 'ँ': 3, 'ं': 4, 'ः': 5, 'अ': 6, 'आ': 7, 'इ': 8, 'ई': 9, 'उ': 10, 'ऊ': 11, 'ऋ': 12, 'ए': 13, 'ऐ': 14, 'ऑ': 15, 'ओ': 16, 'औ': 17, 'क': 18, 'ख': 19, 'ग': 20, 'घ': 21, 'ङ': 22, 'च': 23, 'छ': 24, 'ज': 25, 'झ': 26, 'ञ': 27, 'ट': 28, 'ठ': 29, 'ड': 30, 'ढ': 31, 'ण': 32, 'त': 33, 'थ': 34, 'द': 35, 'ध': 36, 'न': 37, 'प': 38, 'फ': 39, 'ब': 40, 'भ': 41, 'म': 42, 'य': 43, 'र': 44, 'ल': 45, 'ळ': 46, 'व': 47, 'श': 48, 'ष': 49, 'स': 50, 'ह': 51, '़': 52, 'ऽ': 53, 'ा': 54, 'ि': 55, 'ी': 56, 'ु': 57, 'ू': 58, 'ृ': 59, 'ॅ': 60, 'े': 61, 'ै': 62, 'ॉ': 63, 'ॊ': 64, 'ो': 65, 'ौ': 66, '्': 67}\n","Input index Rev {0: '', 1: '^', 2: '$', 3: 'a', 4: 'b', 5: 'c', 6: 'd', 7: 'e', 8: 'f', 9: 'g', 10: 'h', 11: 'i', 12: 'j', 13: 'k', 14: 'l', 15: 'm', 16: 'n', 17: 'o', 18: 'p', 19: 'q', 20: 'r', 21: 's', 22: 't', 23: 'u', 24: 'v', 25: 'w', 26: 'x', 27: 'y', 28: 'z'}\n","Output index Rev {0: '', 1: '^', 2: '$', 3: 'ँ', 4: 'ं', 5: 'ः', 6: 'अ', 7: 'आ', 8: 'इ', 9: 'ई', 10: 'उ', 11: 'ऊ', 12: 'ऋ', 13: 'ए', 14: 'ऐ', 15: 'ऑ', 16: 'ओ', 17: 'औ', 18: 'क', 19: 'ख', 20: 'ग', 21: 'घ', 22: 'ङ', 23: 'च', 24: 'छ', 25: 'ज', 26: 'झ', 27: 'ञ', 28: 'ट', 29: 'ठ', 30: 'ड', 31: 'ढ', 32: 'ण', 33: 'त', 34: 'थ', 35: 'द', 36: 'ध', 37: 'न', 38: 'प', 39: 'फ', 40: 'ब', 41: 'भ', 42: 'म', 43: 'य', 44: 'र', 45: 'ल', 46: 'ळ', 47: 'व', 48: 'श', 49: 'ष', 50: 'स', 51: 'ह', 52: '़', 53: 'ऽ', 54: 'ा', 55: 'ि', 56: 'ी', 57: 'ु', 58: 'ू', 59: 'ृ', 60: 'ॅ', 61: 'े', 62: 'ै', 63: 'ॉ', 64: 'ॊ', 65: 'ो', 66: 'ौ', 67: '्'}\n","Input Data torch.Size([27, 51200])\n","Output Data torch.Size([27, 51200])\n","Input Data Val torch.Size([27, 4096])\n","Output Data Val torch.Size([27, 4096])\n","Input Data Test torch.Size([27, 4096])\n","Output Data Test torch.Size([27, 4096])\n","tensor([21, 10,  3, 21, 22, 20,  3,  9,  3,  3, 20,  2,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0,  0,  0])\n","tensor([ 1, 48, 50, 67, 33, 67, 44, 54, 20, 54, 44,  2,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0,  0,  0])\n"]}],"source":["# dict = {\n","# 'language' : 'hin',\n","# # 'dataset_path' : r'C:\\Users\\gragh\\OneDrive\\Desktop\\Codes\\CS6910 DL\\Assignment 3\\DataSet\\aksharantar_sampled',\n","# 'dataset_path' : '/kaggle/input/dl-ass3/aksharantar_sampled'\n","# }\n","# preprocessed_data = loadData(dict)\n","# tensors = create_tensor(preprocessed_data)\n","\n","# print('Input data : ', preprocessed_data['train_words'])\n","# print('Output data : ', preprocessed_data['train_translations'])\n","# print('Number of samples : ', len(preprocessed_data['train_words']))\n","\n","# print('Input data : ', preprocessed_data['val_words'])\n","# print('Output data : ', preprocessed_data['val_translations'])\n","# print('Number of val samples : ', len(preprocessed_data['val_words']))\n","\n","# print('Input data : ', preprocessed_data['test_words'])\n","# print('Output data : ', preprocessed_data['test_translations'])\n","# print('Number of test samples : ', len(preprocessed_data['test_words']))\n","\n","# print('Max incoder length : ', preprocessed_data['max_enc_len'])\n","# print('Max incoder length : ', preprocessed_data['max_enc_len'])\n","# print('Max length : ', preprocessed_data['max_len'])\n","\n","# print('Input index length', len(preprocessed_data['input_index']))\n","# print('Output index length', len(preprocessed_data['output_index']))\n","# print('Input index', preprocessed_data['input_index'])\n","# print('Output index', preprocessed_data['output_index'])\n","# print('Input index Rev', preprocessed_data['input_index_rev'])\n","# print('Output index Rev', preprocessed_data['output_index_rev'])\n","\n","# print('Input Data', tensors['input_data'].shape)\n","# print('Output Data', tensors['output_data'].shape)\n","# print('Input Data Val', tensors['val_input_data'].shape)\n","# print('Output Data Val', tensors['val_output_data'].shape)\n","# print('Input Data Test', tensors['test_input_data'].shape)\n","# print('Output Data Test', tensors['test_output_data'].shape)\n","\n","# print(tensors['input_data'][:,0])\n","# print(tensors['output_data'][:,0])"]},{"cell_type":"markdown","metadata":{},"source":["# Encoder"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T22:02:00.694556Z","iopub.status.busy":"2024-04-25T22:02:00.693963Z","iopub.status.idle":"2024-04-25T22:02:00.704167Z","shell.execute_reply":"2024-04-25T22:02:00.703347Z","shell.execute_reply.started":"2024-04-25T22:02:00.694524Z"},"trusted":true},"outputs":[],"source":["class Encoder(nn.Module): \n","    def __init__(self, params, preprocessed_data):\n","        super(Encoder, self).__init__()\n","        self.cell_type = params['cell_type']\n","        self.dropout = nn.Dropout(params['dropout'])\n","        self.embedding = nn.Embedding(len(preprocessed_data['input_index']), params['embedding_size'])\n","        if self.cell_type == 'RNN':\n","            self.cell = nn.RNN(params['embedding_size'], params['hidden_size'], params['num_layers_enc'], dropout = params['dropout'], bidirectional = params['bi_dir'])\n","        elif self.cell_type == 'LSTM':\n","            self.cell = nn.LSTM(params['embedding_size'], params['hidden_size'], params['num_layers_enc'], dropout = params['dropout'], bidirectional = params['bi_dir'])\n","        elif self.cell_type == 'GRU':\n","            self.cell = nn.GRU(params['embedding_size'], params['hidden_size'], params['num_layers_enc'], dropout = params['dropout'], bidirectional = params['bi_dir'])\n","        else:\n","            raise ValueError(\"Invalid type. Choose from 'RNN', 'LSTM', or 'GRU'.\")\n","        \n","    def forward(self, x):\n","        drop_par = self.embedding(x)\n","        if self.cell_type == 'LSTM':\n","            outputs , (hidden, cell) = self.cell(self.dropout(drop_par))\n","            return hidden, cell\n","        outputs , hidden = self.cell(self.dropout(drop_par))\n","        return hidden"]},{"cell_type":"markdown","metadata":{},"source":["# Decoder"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T22:02:09.369452Z","iopub.status.busy":"2024-04-25T22:02:09.368421Z","iopub.status.idle":"2024-04-25T22:02:09.381115Z","shell.execute_reply":"2024-04-25T22:02:09.380271Z","shell.execute_reply.started":"2024-04-25T22:02:09.369406Z"},"trusted":true},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, params, preprocessed_data):\n","        super(Decoder, self).__init__()\n","        self.cell_type = params['cell_type']\n","        self.dropout = nn.Dropout(params['dropout'])\n","        self.embedding = nn.Embedding(len(preprocessed_data['output_index']), params['embedding_size'])\n","        if self.cell_type == 'RNN':\n","            self.cell = nn.RNN(params['embedding_size'], params['hidden_size'], params['num_layers_dec'], dropout = params['dropout'], bidirectional = params['bi_dir'])\n","        elif self.cell_type == 'LSTM':\n","            self.cell = nn.LSTM(params['embedding_size'], params['hidden_size'], params['num_layers_dec'], dropout = params['dropout'], bidirectional = params['bi_dir'])\n","        elif self.cell_type == 'GRU':\n","            self.cell = nn.GRU(params['embedding_size'], params['hidden_size'], params['num_layers_dec'], dropout = params['dropout'], bidirectional = params['bi_dir'])\n","        else:\n","            raise ValueError(\"Invalid type. Choose from 'RNN', 'LSTM', or 'GRU'.\")\n","        \n","        self.fc = nn.Linear(params['hidden_size'] * 2 if params['bi_dir'] == True else params['hidden_size'], len(preprocessed_data['output_index']))\n","\n","    def forward(self, x, hidden, cell):\n","        embedding = self.embedding(x.unsqueeze(0))\n","        if self.cell_type == 'LSTM':\n","            outputs, (hidden, cell) = self.cell(self.dropout(embedding), (hidden, cell))\n","        else:    \n","            outputs, hidden = self.cell(self.dropout(embedding), hidden)\n","        predictions = self.fc(outputs).squeeze(0)\n","        if self.cell_type == 'LSTM':\n","            predictions = F.log_softmax(predictions, dim = 1)\n","            return predictions, hidden, cell\n","        return predictions, hidden"]},{"cell_type":"markdown","metadata":{},"source":["# Seq2Seq"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T22:02:11.341014Z","iopub.status.busy":"2024-04-25T22:02:11.340658Z","iopub.status.idle":"2024-04-25T22:02:11.350268Z","shell.execute_reply":"2024-04-25T22:02:11.349367Z","shell.execute_reply.started":"2024-04-25T22:02:11.340986Z"},"trusted":true},"outputs":[],"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, params,  preprocessed_data):\n","        super(Seq2Seq, self).__init__()\n","        self.cell_type = params['cell_type']\n","        self.decoder, self.encoder  = decoder, encoder\n","        self.output_index_len = len(preprocessed_data['output_index'])\n","        self.tfr = params['teacher_fr']\n","\n","    def forward(self, source, target):\n","        batch_size, target_len = source.shape[1], target.shape[0]\n","        x = target[0]\n","        outputs = torch.zeros(target_len, batch_size, self.output_index_len).to(device)\n","        if self.cell_type == 'LSTM':\n","            hidden, cell = self.encoder(source)\n","        else:    \n","            hidden = self.encoder(source)\n","        for t in range(1, target_len):\n","            if self.cell_type == 'LSTM':\n","                output, hidden, cell = self.decoder(x, hidden, cell)\n","            else:    \n","                output, hidden = self.decoder(x, hidden, None)\n","            outputs[t], best_guess = output, output.argmax(1)\n","            x = best_guess if random.random() >= self.tfr else target[t]\n","        return outputs"]},{"cell_type":"markdown","metadata":{},"source":["# GET OPTIMIZERS"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T22:02:11.831253Z","iopub.status.busy":"2024-04-25T22:02:11.830929Z","iopub.status.idle":"2024-04-25T22:02:11.838599Z","shell.execute_reply":"2024-04-25T22:02:11.837718Z","shell.execute_reply.started":"2024-04-25T22:02:11.831226Z"},"trusted":true},"outputs":[],"source":["def get_optim(model, params):\n","    if params['optimizer'].lower() == 'sgd':\n","        optimizer = optim.SGD(model.parameters(), lr = params['learning_rate'], momentum = 0.9)\n","    if params['optimizer'].lower() == 'adam':\n","        optimizer = optim.Adam(model.parameters(), lr = params['learning_rate'], betas = (0.9, 0.999), eps = 1e-8)\n","    if params['optimizer'].lower() == 'rmsprop':\n","        optimizer = optim.RMSprop(model.parameters(), lr = params['learning_rate'], alpha = 0.99, eps = 1e-8)\n","    if params['optimizer'].lower() == 'adagrad':\n","        optimizer = optim.Adagrad(model.parameters(), lr = params['learning_rate'], lr_decay = 0, weight_decay = 0, initial_accumulator_value = 0, eps = 1e-10)\n","    return optimizer"]},{"cell_type":"markdown","metadata":{},"source":["# GET TOTAL PARAMETERS"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T22:02:12.415828Z","iopub.status.busy":"2024-04-25T22:02:12.415216Z","iopub.status.idle":"2024-04-25T22:02:12.420295Z","shell.execute_reply":"2024-04-25T22:02:12.419329Z","shell.execute_reply.started":"2024-04-25T22:02:12.415790Z"},"trusted":true},"outputs":[],"source":["def get_total_parameters(model):\n","    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    return total_params"]},{"cell_type":"markdown","metadata":{},"source":["# BEAM SEARCH"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T22:02:13.076666Z","iopub.status.busy":"2024-04-25T22:02:13.076077Z","iopub.status.idle":"2024-04-25T22:02:13.091745Z","shell.execute_reply":"2024-04-25T22:02:13.090746Z","shell.execute_reply.started":"2024-04-25T22:02:13.076630Z"},"trusted":true},"outputs":[],"source":["def beam_search(model, word, preprocessed_data, bw, lp, ct):\n","    data = np.zeros((preprocessed_data['max_len']+1, 1), dtype=np.int32)\n","    for idx, char in enumerate(word):\n","        data[idx, 0] = preprocessed_data['input_index'][char]\n","    data[idx + 1, 0] = preprocessed_data['input_index'][preprocessed_data['EOS']]\n","    data = torch.tensor(data, dtype=torch.int32).to(device)\n","    with torch.no_grad():\n","        if ct == 'LSTM':\n","           hidden, cell = model.encoder(data)\n","        else:\n","           hidden = model.encoder(data)\n","    output_start = preprocessed_data['output_index'][preprocessed_data['SOS']]\n","    out_reshape = np.array(output_start).reshape(1,)\n","    hidden_par = hidden.unsqueeze(0)\n","    initial_sequence = torch.tensor(out_reshape).to(device)\n","    beam = [(0.0, initial_sequence, hidden_par)]\n","    for i in range(len(preprocessed_data['output_index'])):\n","        candidates = []\n","        for score, seq, hidden in beam:\n","            if seq[-1].item() == preprocessed_data['output_index'][preprocessed_data['EOS']]:\n","                candidates.append((score, seq, hidden))\n","                continue\n","            reshape_last = np.array(seq[-1].item()).reshape(1, )\n","            hdn = hidden.squeeze(0) \n","            x = torch.tensor(reshape_last).to(device)\n","            if ct == 'LSTM':\n","                output, hidden, cell = model.decoder(x, hdn, cell)\n","            else:\n","                output, hidden = model.decoder(x, hdn, None)\n","            topk_probs, topk_tokens = torch.topk(F.softmax(output, dim=1), k = bw)               \n","            for prob, token in zip(topk_probs[0], topk_tokens[0]):\n","                new_seq = torch.cat((seq, token.unsqueeze(0)), dim=0)\n","                ln_ns = len(new_seq)\n","                ln_pf = ((ln_ns - 1) / 5)\n","                candidate_score = score + torch.log(prob).item() / (ln_pf ** lp)\n","                candidates.append((candidate_score, new_seq, hidden.unsqueeze(0)))\n","        beam = heapq.nlargest(bw, candidates, key=lambda x: x[0])\n","    _, best_sequence, _ = max(beam, key=lambda x: x[0]) \n","    prediction = ''.join([preprocessed_data['output_index_rev'][token.item()] for token in best_sequence[1:]])\n","    return prediction[:-1]          \n"]},{"cell_type":"markdown","metadata":{},"source":["# TRAIN MODEL"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T22:02:13.744780Z","iopub.status.busy":"2024-04-25T22:02:13.744471Z","iopub.status.idle":"2024-04-25T22:02:13.763114Z","shell.execute_reply":"2024-04-25T22:02:13.762243Z","shell.execute_reply.started":"2024-04-25T22:02:13.744755Z"},"trusted":true},"outputs":[],"source":["def train(model, criterion, optimizer, preprocessed_data, tensors, params):\n","    train_data, train_result = torch.split(tensors['input_data'], params['batch_size'], dim = 1), torch.split(tensors['output_data'], params['batch_size'], dim = 1)\n","    val_data, val_result = torch.split(tensors['val_input_data'], params['batch_size'], dim=1), torch.split(tensors['val_output_data'], params['batch_size'], dim=1)\n","    for epoch in range(params['num_epochs']):\n","        total_words = 0\n","        correct_pred = 0\n","        total_loss = 0\n","        model.train()\n","        with tqdm(total = len(train_data), desc = 'Training') as pbar:\n","            for i, (x, y) in enumerate(zip(train_data, train_result)):\n","                target, inp_data = y.to(device), x.to(device)\n","                optimizer.zero_grad()\n","                output = model(inp_data, target)\n","                target = target.reshape(-1)\n","                output = output.reshape(-1, output.shape[2])\n","            \n","#                 pad_mask = (target != preprocessed_data['output_index'][preprocessed_data['PAD']])\n","#                 target = target[pad_mask]\n","#                 output = output[pad_mask]\n","                \n","                loss = criterion(output, target)\n","                loss.backward()\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","                optimizer.step()\n","                total_loss += loss.item()\n","                total_words += target.size(0)\n","                correct_pred += torch.sum(torch.argmax(output, dim=1) == target).item()\n","                pbar.update(1)\n","        train_accuracy = (correct_pred / total_words)*100\n","        train_loss = total_loss / len(train_data)\n","        model.eval()\n","        with torch.no_grad():\n","            val_total_loss = 0\n","            val_total_words = 0\n","            val_correct_pred = 0\n","            with tqdm(total = len(val_data), desc = 'Validation') as pbar:\n","                for x_val, y_val in zip(val_data, val_result):\n","                    target_val, inp_data_val = y_val.to(device), x_val.to(device)\n","                    output_val = model(inp_data_val, target_val)\n","                    target_val = target_val.reshape(-1)\n","                    output_val = output_val.reshape(-1, output_val.shape[2])\n","                    \n","#                     pad_mask = (target_val != preprocessed_data['output_index'][preprocessed_data['PAD']])\n","#                     target_val = target_val[pad_mask]\n","#                     output_val = output_val[pad_mask]\n","                    \n","                    val_loss = criterion(output_val, target_val)\n","                    val_total_loss += val_loss.item()\n","                    val_total_words += target_val.size(0)\n","                    val_correct_pred += torch.sum(torch.argmax(output_val, dim=1) == target_val).item()\n","                    pbar.update(1)\n","            val_accuracy = (val_correct_pred / val_total_words) * 100\n","            val_loss = val_total_loss / len(val_data)\n","            \n","            correct_pred = 0\n","            total_words = len(preprocessed_data['val_words'])\n","            with tqdm(total = total_words, desc = 'Beam') as pbar_:\n","                for word, translation in zip(preprocessed_data['val_words'], preprocessed_data['val_translations']):\n","                    ans = beam_search(model, word, preprocessed_data, params['beam_width'], params['length_penalty'], params['cell_type'])\n","                    if ans == translation[1:-1]:\n","                        correct_pred += 1\n","                    pbar_.update(1)\n","        val_accuracy_beam = (correct_pred / total_words) * 100\n","        print(f'''Epoch : {epoch+1}\n","              Train Accuracy : {train_accuracy:.4f}, Train Loss : {train_loss:.4f}\n","              Validation Accuracy Char Level : {val_accuracy:.4f}, Validation Loss : {val_loss:.4f}\n","              Validation Accuracy Word Level : {val_accuracy_beam:.4f},  Correctly predicted : {correct_pred}/{total_words}''')\n","        if params['w_log']:\n","            wandb.log(\n","                    {\n","                        'epoch': epoch+1,\n","                        'training_loss' : train_loss,\n","                        'training_accuracy' : train_accuracy,\n","                        'validation_loss' : val_loss,\n","                        'validation_accuracy_char' : val_accuracy,\n","                        'validation_accuracy_word' : val_accuracy_beam,\n","                        'correctly_predicted' : correct_pred\n","                    }\n","                )\n","    return model, val_accuracy, val_accuracy_beam"]},{"cell_type":"markdown","metadata":{},"source":["# QUESTION 1"]},{"cell_type":"markdown","metadata":{},"source":["# HYPERPARAMETERS"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T22:02:14.479530Z","iopub.status.busy":"2024-04-25T22:02:14.478887Z","iopub.status.idle":"2024-04-25T22:04:41.313372Z","shell.execute_reply":"2024-04-25T22:04:41.312440Z","shell.execute_reply.started":"2024-04-25T22:02:14.479498Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 800/800 [00:36<00:00, 22.02it/s]\n","Validation: 100%|██████████| 64/64 [00:01<00:00, 63.08it/s]\n","Beam: 100%|██████████| 4096/4096 [00:34<00:00, 118.56it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 1\n","              Train Accuracy : 18.5803, Train Loss : 1.7459\n","              Validation Accuracy Char Level : 21.1010, Validation Loss : 1.2179\n","              Validation Accuracy Word Level : 22.0947,  Correctly predicted : 905/4096\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 800/800 [00:35<00:00, 22.32it/s]\n","Validation: 100%|██████████| 64/64 [00:01<00:00, 60.37it/s]\n","Beam: 100%|██████████| 4096/4096 [00:34<00:00, 118.38it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch : 2\n","              Train Accuracy : 24.1999, Train Loss : 1.1777\n","              Validation Accuracy Char Level : 22.3615, Validation Loss : 1.0858\n","              Validation Accuracy Word Level : 29.5654,  Correctly predicted : 1211/4096\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["params = {\n","#     'dataset_path' : r'C:\\Users\\gragh\\OneDrive\\Desktop\\Codes\\CS6910 DL\\Assignment 3\\DataSet\\aksharantar_sampled',\n","    'language' : 'hin',\n","    'dataset_path' : '/kaggle/input/dl-ass3/aksharantar_sampled',\n","    'embedding_size': 254,\n","    'hidden_size': 256,\n","    'num_layers_enc': 2,\n","    'num_layers_dec': 2,\n","    'cell_type': 'LSTM',\n","    'dropout': 0.5,\n","    'optimizer' : 'adam',\n","    'learning_rate': 0.001,\n","    'batch_size': 64,\n","    'num_epochs': 2,\n","    'teacher_fr' : 0.7,\n","    'length_penalty' : 0.6,\n","    'beam_width': 1,\n","    'bi_dir' : True,\n","    'w_log' : 0\n","}\n","preprocessed_data = loadData(params)\n","tensors = create_tensor(preprocessed_data)\n","\n","encoder = Encoder(params, preprocessed_data).to(device)\n","decoder = Decoder(params, preprocessed_data).to(device)\n","model = Seq2Seq(encoder, decoder, params, preprocessed_data).to(device)  \n","# print(model)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index = 0)\n","optimizer = get_optim(model,params)\n","# Print total number of parameters in the model\n","# total_parameters = get_total_parameters(model)\n","# print(f'Total Trainable Parameters: {total_parameters}')\n","\n","if params['w_log']:\n","    wandb.init(project = 'DL-Assignment-3')\n","    wandb.run.name = (\n","        'check_c:' + params['cell_type'] +\n","        '_e:' + str(params['num_epochs']) +\n","        '_es:' + str(params['embedding_size']) +\n","        '_hs:' + str(params['hidden_size']) +\n","        '_nle:' + str(params['num_layers_enc']) +\n","        '_nld:' + str(params['num_layers_dec']) +\n","        '_o:' + params['optimizer'] +\n","        '_lr:' + str(params['learning_rate']) +\n","        '_bs:' + str(params['batch_size']) +\n","        '_tf:' + str(params['teacher_fr']) +\n","        '_lp:' + str(params['length_penalty']) +\n","        '_b:' + str(params['bi_dir']) +\n","        '_bw:' + str(params['beam_width'])\n","    )\n","trained_model, _, _ = train(model, criterion, optimizer, preprocessed_data, tensors, params)\n","if params['w_log']:\n","    wandb.finish()"]},{"cell_type":"markdown","metadata":{},"source":["# Prediction"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T22:04:41.315365Z","iopub.status.busy":"2024-04-25T22:04:41.314919Z","iopub.status.idle":"2024-04-25T22:04:41.819890Z","shell.execute_reply":"2024-04-25T22:04:41.819000Z","shell.execute_reply.started":"2024-04-25T22:04:41.315333Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["################################## using predict ############################################################\n","harsh -> हर्श\n","iit -> आईआईटी\n","madras -> मद्रस\n","nirav -> निरवव\n","nishchal -> निश्चल\n","nishant -> निशांत\n","neymar -> नेयामर\n","neha -> नेहा\n","raghav -> रघवव\n","rahul -> रहुल\n","rohit -> रोहित\n","hahahahaha -> हहहहहाहा\n","ohohohoh -> ओहोहोहोह\n","mera -> मेरा\n","naam -> नाम\n","raghav -> रघवव\n","hai -> है\n","to -> टीओए\n","chaliye -> चलिये\n","shuru -> शुरू\n","krte -> क्रेट\n","hai -> है\n","bina -> बिना\n","kisi -> किसी\n","bakchodi -> बकचोड़ी\n","ke -> के\n","jaisawal$ -> जैसवाल\n","bajai$ -> बजाई\n","sanghthan$ -> संघथनन\n","haiwaan$ -> हैवां\n","nilgiri$ -> निलगिरी\n","drutgrami$ -> द्रुत्ग्रमी\n","jhadapon$ -> झड़पों\n","nakronda$ -> नकरोंडा\n","eesl$ -> इईसल\n","bachta$ -> बच्ता\n","################################## using beam ############################################################\n","harsh -> हर्श\n","iit -> आईआईटी\n","madras -> मद्रस\n","nirav -> निरवव\n","nishchal -> निश्चल\n","nishant -> निशांत\n","neymar -> नेयामर\n","neha -> नेहा\n","raghav -> रघवव\n","rahul -> रहुल\n","rohit -> रोहित\n","hahahahaha -> हहहहहाहा\n","ohohohoh -> ओहोहोहोह\n","mera -> मेरा\n","naam -> नाम\n","raghav -> रघवव\n","hai -> है\n","to -> टीओए\n","chaliye -> चलिये\n","shuru -> शुरू\n","krte -> क्रेट\n","hai -> है\n","bina -> बिना\n","kisi -> किसी\n","bakchodi -> बकचोड़ी\n","ke -> के\n","jaisawal$ -> जैसवाल\n","bajai$ -> बजाई\n","sanghthan$ -> संघथनन\n","haiwaan$ -> हैवां\n","nilgiri$ -> निलगिरी\n","drutgrami$ -> द्रुत्ग्रमी\n","jhadapon$ -> झड़पों\n","nakronda$ -> नकरोंडा\n","eesl$ -> इईसल\n","bachta$ -> बच्ता\n"]}],"source":["def predict(model, word, preprocessed_data, params):\n","    data = np.zeros((len(preprocessed_data['input_index']),1), dtype= int)\n","    pred = ''\n","    for t, char in enumerate(word):\n","        data[t, 0] = preprocessed_data['input_index'][char]\n","    data[(t+1),0] = preprocessed_data['input_index'][preprocessed_data['EOS']]\n","    data = torch.tensor(data,dtype = torch.int64).to(device)\n","    with torch.no_grad():\n","        if params['cell_type'] == 'LSTM':\n","            hidden, cell = model.encoder(data)\n","        else:\n","            hidden = model.encoder(data)\n","    sos_reshape = np.array(preprocessed_data['output_index'][preprocessed_data['SOS']]).reshape(1,)    \n","    x = torch.tensor(sos_reshape).to(device)\n","    for t in range(1, len(preprocessed_data['output_index'])):\n","        if params['cell_type'] == 'LSTM':\n","            output, hidden, cell = model.decoder(x, hidden, cell)\n","        else:\n","            output, hidden = model.decoder(x, hidden, None)\n","        character = preprocessed_data['output_index_rev'][output.argmax(1).item()]\n","        if character != preprocessed_data['EOS']:\n","            pred = pred + character\n","        else:\n","            break\n","        x = torch.tensor(output.argmax(1)).to(device)        \n","    return pred\n","\n","words = ['harsh', 'iit', 'madras', 'nirav', 'nishchal', 'nishant', 'neymar', 'neha', 'raghav', 'rahul', 'rohit', 'hahahahaha', 'ohohohoh']\n","words2 = ['mera', 'naam', 'raghav', 'hai', 'to', 'chaliye', 'shuru', 'krte' ,'hai', 'bina', 'kisi', 'bakchodi', 'ke']\n","print('################################## using predict function ############################################################')\n","for w in words:\n","    output_sequence = predict(trained_model, w, preprocessed_data, params)\n","    print(w,'->',output_sequence)\n","for w in words2:\n","    output_sequence = predict(trained_model, w, preprocessed_data, params)\n","    print(w,'->',output_sequence)\n","for w in preprocessed_data['val_words'][:10]:\n","    output_sequence = predict(trained_model, w[:-1], preprocessed_data, params)\n","    print(w,'->',output_sequence)\n","print('################################## using beam search ############################################################')\n","for w in words:\n","    output_sequence = beam_search(trained_model, w, preprocessed_data, params['beam_width'], params['length_penalty'], params['cell_type'])\n","    print(w,'->',output_sequence)\n","for w in words2:\n","    output_sequence = beam_search(trained_model, w, preprocessed_data, params['beam_width'], params['length_penalty'], params['cell_type'])\n","    print(w,'->',output_sequence)\n","for w in preprocessed_data['val_words'][:10]:\n","    output_sequence = beam_search(trained_model, w[:-1], preprocessed_data, params['beam_width'], params['length_penalty'], params['cell_type'])\n","    print(w,'->',output_sequence)        \n"]},{"cell_type":"markdown","metadata":{},"source":["# QUESTION 2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# sweep_config = {\n","#             'name': 'sweep 1 and 1.1 : random',\n","#             'method': 'random',\n","#             'metric': { 'goal': 'maximize','name': 'Accuracy'},\n","#             'parameters': \n","#                 {\n","#                     'num_epochs': {'values': [10]},\n","#                     'cell_type': {'values': ['RNN', 'LSTM', 'GRU']},\n","#                     'embedding_size': {'values': [128, 256, 512]},\n","#                     'hidden_size': {'values': [128, 256, 512]},\n","#                     'num_layers': {'values': [1, 2, 3]},\n","#                     'dropout': {'values': [0.3, 0.5, 0.7]},\n","#                     'optimizer' : {'values' : ['adam', 'sgd', 'rmsprop', 'adagrad']},\n","#                     'learning_rate': {'values': [0.001, 0.005, 0.01, 0.1]},\n","#                     'batch_size': {'values': [32, 64]},\n","#                     'teacher_fr' : {'values': [0.3, 0.5, 0.7]},\n","#                     'length_penalty' : {'values': [0.4, 0.5, 0.6]},\n","#                     'bi_dir' : {'values': [True, False]},\n","#                     'beam_width': {'values': [1, 2, 3]}\n","#                 }\n","#             }"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# sweep_config = {\n","#             'name': 'sweep 2 : bayes',\n","#             'method': 'bayes',\n","#             'metric': { 'goal': 'maximize','name': 'Accuracy'},\n","#             'parameters': \n","#                 {\n","#                     'num_epochs': {'values': [10]},\n","#                     'cell_type': {'values': ['LSTM', 'GRU']},\n","#                     'embedding_size': {'values': [128, 256]},\n","#                     'hidden_size': {'values': [128, 256, 512]},\n","#                     'num_layers': {'values': [1, 2, 3]},\n","#                     'dropout': {'values': [0.3, 0.5]},\n","#                     'optimizer' : {'values' : ['adam']},\n","#                     'learning_rate': {'values': [0.001, 0.005, 0.01, 0.1]},\n","#                     'batch_size': {'values': [32, 64]},\n","#                     'teacher_fr' : {'values': [0.3, 0.5, 0.7]},\n","#                     'length_penalty' : {'values': [0.5, 0.6]},\n","#                     'bi_dir' : {'values': [True]},\n","#                     'beam_width': {'values': [1]}\n","#                 }\n","#             }"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def run_sweep():\n","    init = wandb.init(project = 'DL-Assignment-3')\n","    config = init.config\n","    params = {\n","        'language' : 'hin',\n","        'dataset_path' : '/kaggle/input/dl-ass3/aksharantar_sampled',\n","        'num_epochs': config.num_epochs,\n","        'cell_type': config.cell_type,\n","        'embedding_size': config.embedding_size,\n","        'hidden_size': config.hidden_size,\n","        'num_layers_enc': config.num_layers,\n","        'num_layers_dec': config.num_layers,\n","        'dropout': config.dropout,\n","        'optimizer' : config.optimizer,\n","        'learning_rate': config.learning_rate,\n","        'batch_size': config.batch_size,\n","        'teacher_fr' : config.teacher_fr,\n","        'length_penalty' : config.length_penalty,\n","        'bi_dir' : config.bi_dir,\n","        'beam_width' : config.beam_width,\n","        'w_log' : 1\n","    }\n","    \n","    wandb.run.name = (\n","        'Q2_c:' + params['cell_type'] +\n","        '_e' + str(params['num_epochs']) +\n","        '_es:' + str(params['embedding_size']) +\n","        '_hs:' + str(params['hidden_size']) +\n","        '_nle:' + str(params['num_layers_enc']) +\n","        '_nld:' + str(params['num_layers_dec']) +\n","        '_o:' + params['optimizer'] +\n","        '_lr:' + str(params['learning_rate']) +\n","        '_bs:' + str(params['batch_size']) +\n","        '_tf:' + str(params['teacher_fr']) +\n","        '_lp:' + str(params['length_penalty']) +\n","        '_b:' + str(params['bi_dir']) +\n","        '_bw:' + str(params['beam_width'])\n","    )\n","    preprocessed_data = loadData(params)\n","    tensors = create_tensor(preprocessed_data)\n","    \n","    encoder = Encoder(params, preprocessed_data).to(device)\n","    decoder = Decoder(params, preprocessed_data).to(device)\n","    model = Seq2Seq(encoder, decoder, params, preprocessed_data).to(device) \n","    \n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = get_optim(model,params)\n","    _, _, v_acc_beam = train(model, criterion, optimizer, preprocessed_data, tensors, params)\n","    wandb.log({'Accuracy': v_acc_beam})"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# sweep_id = wandb.sweep(sweep_config, project='DL-Assignment-3')\n","# wandb.agent(sweep_id, run_sweep, count = 30)\n","# wandb.finish()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4855222,"sourceId":8196746,"sourceType":"datasetVersion"},{"datasetId":4868284,"sourceId":8213932,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
