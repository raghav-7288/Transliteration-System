{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(train_path, val_path, test_path):\n",
    "    train_data = csv.reader(open(train_path,encoding='utf8'))\n",
    "    val_data = csv.reader(open(val_path,encoding='utf8'))\n",
    "    test_data = csv.reader(open(test_path,encoding='utf8'))\n",
    "    train_words , train_translations = [], []\n",
    "    val_words , val_translations = [], []\n",
    "    test_words , test_translations = [], []\n",
    "    \n",
    "    for pair in train_data:\n",
    "        train_words.append(pair[0])\n",
    "        train_translations.append(pair[1])\n",
    "    for pair in val_data:\n",
    "        val_words.append(pair[0])\n",
    "        val_translations.append(pair[1])\n",
    "    for pair in test_data:\n",
    "        test_words.append(pair[0])\n",
    "        test_translations.append(pair[1])\n",
    "    \n",
    "    train_words , train_translations = np.array(train_words), np.array(train_translations)\n",
    "    val_words , val_translations = np.array(val_words), np.array(val_translations)\n",
    "    test_words , test_translations = np.array(test_words), np.array(test_translations)\n",
    "    \n",
    "    input_tokens, output_tokens = set(), set()\n",
    "    val_input_tokens, val_output_tokens = set(), set()\n",
    "    \n",
    "    for word in train_words:\n",
    "        for character in word:\n",
    "            if character not in input_tokens:\n",
    "                input_tokens.add(character)\n",
    "    for word in train_translations:\n",
    "        for character in word:\n",
    "            if character not in output_tokens:\n",
    "                output_tokens.add(character)\n",
    "\n",
    "    input_tokens.add(' ')\n",
    "    output_tokens.add(' ')\n",
    "    input_tokens,  output_tokens = sorted(list(input_tokens)), sorted(list(output_tokens))\n",
    "    \n",
    "    for word in val_words:\n",
    "        for character in word:\n",
    "            if character not in val_input_tokens:\n",
    "                val_input_tokens.add(character)\n",
    "    for word in val_translations:\n",
    "        for character in word:\n",
    "            if character not in val_output_tokens:\n",
    "                val_output_tokens.add(character)\n",
    "                \n",
    "    result = {\n",
    "        'train_words' : train_words,\n",
    "        'train_translations' : train_translations,\n",
    "        'val_words' : val_words,\n",
    "        'val_translations' : val_translations,\n",
    "        'test_words' : test_words,\n",
    "        'test_translations' : test_translations,\n",
    "        'input_tokens' : input_tokens,\n",
    "        'output_tokens' : output_tokens,\n",
    "        'val_input_tokens' : val_input_tokens,\n",
    "        'val_output_tokens' : val_output_tokens\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_tensor(result):\n",
    "    max_input_length = max([len(word) for word in result['train_words']]) + 2\n",
    "    max_output_length = max([len(word) for word in result['train_translations']])\n",
    "    max_input_length_val = max([len(word) for word in result['val_words']]) + 2\n",
    "    max_output_length_val = max([len(word) for word in result['val_translations']])\n",
    "    \n",
    "    input_index = dict([(char, idx) for idx, char in enumerate(result['input_tokens'])])\n",
    "    output_index =  dict([(char, idx) for idx, char in enumerate(result['output_tokens'])])\n",
    "    input_index_rev = dict([(idx, char) for char, idx in input_index.items()])\n",
    "    output_index_rev = dict([(idx, char) for char, idx in output_index.items()])\n",
    "    index_dict = {\n",
    "        'input_index' : input_index,\n",
    "        'output_index' : output_index,\n",
    "        'input_index_rev' : input_index_rev,\n",
    "        'output_index_rev' : output_index_rev\n",
    "    }\n",
    "\n",
    "    input_data = np.zeros((max_input_length,len(result['train_words'])), dtype = 'int64')\n",
    "    output_data = np.zeros((max_output_length,len(result['train_words'])), dtype = 'int64')\n",
    "    \n",
    "    val_input_data = np.zeros((max_input_length_val,len(result['val_words'])), dtype = 'int64')\n",
    "    val_output_data = np.zeros((max_output_length_val,len(result['val_words'])), dtype = 'int64')\n",
    "    \n",
    "    for idx, (w, t) in enumerate(zip(result['train_words'], result['train_translations'])):\n",
    "        for i, char in enumerate(w):\n",
    "            input_data[i, idx] = input_index[char]\n",
    "        input_data[i+1 :,idx] = input_index[\" \"]\n",
    "        for i, char in enumerate(t):\n",
    "            output_data[i, idx] = output_index[char]\n",
    "        output_data[i+1 :,idx] = output_index[\" \"]\n",
    "        \n",
    "    for idx, (w, t) in enumerate(zip(result['val_words'], result['val_translations'])):\n",
    "        for i, char in enumerate(w):\n",
    "            val_input_data[i, idx] = input_index[char]\n",
    "        val_input_data[i+1 :,idx] = input_index[\" \"]\n",
    "        for i, char in enumerate(t):\n",
    "            val_output_data[i, idx] = output_index[char]\n",
    "        val_output_data[i+1 :,idx] = output_index[\" \"]\n",
    "    \n",
    "    input_data, output_data = torch.tensor(input_data,dtype = torch.int64), torch.tensor(output_data, dtype = torch.int64)\n",
    "    val_input_data, val_output_data = torch.tensor(val_input_data,dtype = torch.int64), torch.tensor(val_output_data, dtype = torch.int64)\n",
    "    \n",
    "    return input_data, output_data, val_input_data, val_output_data, index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'hin'\n",
    "dataset_path = r'C:\\Users\\gragh\\OneDrive\\Desktop\\Codes\\CS6910 DL\\Assignment 3\\DataSet\\aksharantar_sampled'\n",
    "train_path = os.path.join(dataset_path, language, language + '_train.csv')\n",
    "val_path = os.path.join(dataset_path, language, language + '_valid.csv')\n",
    "test_path = os.path.join(dataset_path, language, language + '_test.csv')\n",
    "\n",
    "result = loadData(train_path, val_path, test_path)\n",
    "input_data, output_data, val_input_data, val_output_data, index_dict = create_tensor(result)\n",
    "\n",
    "# num_samples = len(result['train_words'])\n",
    "# num_input_tokens = len(result['input_tokens'])\n",
    "# num_output_tokens = len(result['output_tokens'])\n",
    "# num_val_input_tokens = len(result['val_input_tokens'])\n",
    "# num_val_output_tokens = len(result['val_output_tokens'])\n",
    "# max_input_length_train = max([len(word) for word in result['train_words']]) + 2\n",
    "# max_output_length_train = max([len(word) for word in result['train_translations']])\n",
    "# max_input_length_val = max([len(word) for word in result['val_words']]) + 2\n",
    "# max_output_length_val = max([len(word) for word in result['val_translations']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Number of samples : ', num_samples)\n",
    "# print('Number of unique train input tokens : ', num_input_tokens)\n",
    "# print('Number of unique train output tokens : ', num_output_tokens)\n",
    "# print('Number of unique val input tokens : ', num_val_input_tokens)\n",
    "# print('Number of unique val output tokens : ', num_val_output_tokens)\n",
    "# print('Max sequence length for inputs : ', max_input_length_train)\n",
    "# print('Max sequence length for outputs : ', max_output_length_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
